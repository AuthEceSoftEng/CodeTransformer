{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNifeFm7vVo",
        "colab_type": "text"
      },
      "source": [
        "Installing the wget package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGYO-apCYXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rZDo54s7vqv",
        "colab_type": "text"
      },
      "source": [
        "Importing useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owo6okSwBFtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import wget\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2oKoGnS7wK3",
        "colab_type": "text"
      },
      "source": [
        "Downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCToFP_NBtbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip'\n",
        "wget.download(url, '/content/dataset.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hl4n5mc7wxu",
        "colab_type": "text"
      },
      "source": [
        "Unzipping the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvXF3LTbCt3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/dataset.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mVkkAVB7xSR",
        "colab_type": "text"
      },
      "source": [
        "Unpickling the dataset and creating a DataFrame with its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduRE7TVSnQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pickle.load(open('java_dedupe_definitions_v2.pkl', 'rb'))\n",
        "data = pd.DataFrame(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDwI5JcNFuo2",
        "colab_type": "text"
      },
      "source": [
        "Removing data without a docstring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVI3TqsG98d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing rows with no docstrings.\n",
        "data = data[data['docstring_tokens'].map(lambda d: len(d)) > 0]\n",
        "# resetting DataFrame indices.\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# selecting only the first 50,000 rows for faster testing.\n",
        "data = data[:50000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWRBc2Py7x1-",
        "colab_type": "text"
      },
      "source": [
        "Defining the preprocessing functions.\n",
        "\n",
        "*   *remove_after_dot*: removes all strings after the occurence of the first dot in the doscstring.\n",
        "*   *remove_non_ascii*: replaces all non-ASCII characters with an empty string.\n",
        "*   *remove_special*: replaces all special characters in the docstring with an empty string.\n",
        "*   *seperate_strings*: seperates strings that have at least one uppercase and one lowercase letter.\n",
        "*   *remove_empty*: removes all empty strings.\n",
        "*   *fill_empty*: empties docstrings with less than three or more than 30 words and fills them with words from function's identifier to perform data augmentation.\n",
        "*   *lowercase*: lowercases all strings in the docstring to avoid case sensitivity.\n",
        "*   *remove_comments*: removes all comments in the function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oUIVlg3363f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_after_dot(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      if token == '.':\n",
        "        token_index = row.index(token)\n",
        "        data[index] = row[:token_index]\n",
        "        break\n",
        "  \n",
        "  return data\n",
        "\n",
        "def remove_non_ascii(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing non-ASCII characters with an empty string.\n",
        "      token = re.sub(r'[^\\x00-\\x7f]', '', token)\n",
        "      data[index][token_index] = token\n",
        "      \n",
        "  return data\n",
        "\n",
        "def remove_special(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def seperate_strings(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      # if the string has at least one uppercase and one lowercase letter.\n",
        "      if re.findall(r'[A-Z][a-z][^A-Z]*', token):\n",
        "        token_index = row.index(token)\n",
        "        # capitalizing the first letter of the token.\n",
        "        token = token[0].capitalize() + token[1:]\n",
        "        token = re.findall(r'[A-Z][a-z][^A-Z]*', token)\n",
        "        # replacing token with an empty string.\n",
        "        data[index][token_index] = ''\n",
        "        # adding the seperated words to the list preserving their original position.\n",
        "        data[index] = data[index][:token_index] + token + data[index][token_index:]\n",
        "        # updating row.\n",
        "        row = data[index]\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_empty(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      if not token:  \n",
        "        # removing empty strings from the list.\n",
        "        data[index] = list(filter(None, row))\n",
        "\n",
        "  return data\n",
        "\n",
        "def fill_empty(identifier, data):\n",
        "  for (index, row), identifier_row in zip(data.iteritems(), identifier):\n",
        "    if len(row) < 3 or len(row) > 30:\n",
        "        data[index] = []\n",
        "    if not data[index]:\n",
        "      # splitting identifiers on the dots.\n",
        "      augmented_row = identifier_row.split('.')\n",
        "      # capitalizing the first letter of the second half of the identifier.\n",
        "      augmented_row[1] = augmented_row[1][0].capitalize() + augmented_row[1][1:]\n",
        "      # seperating all identifier words using their first capital letter.\n",
        "      a = re.findall(r'[A-Z][^A-Z]*', augmented_row[0])\n",
        "      b = re.findall(r'[A-Z][^A-Z]*', augmented_row[1])\n",
        "      data[index] = a + b\n",
        "\n",
        "  return data\n",
        "\n",
        "def lowercase(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      token = token.lower()\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_comments(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      if re.findall(r'(^//)', token):\n",
        "        token_index = row.index(token)\n",
        "        # replacing token with an empty string.\n",
        "        data[index][token_index] = ''\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RenAKRKM7ypZ",
        "colab_type": "text"
      },
      "source": [
        "Applying preprocressing to the docstring tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPFG6PIUUWu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copying docstring_tokens column.\n",
        "docstring_tokens = data['docstring_tokens'].copy(deep=True)\n",
        "# copying identifier column.\n",
        "identifier = data['identifier'].copy(deep=True)\n",
        "\n",
        "# applying the preprocessing functions on all docstring tokens.\n",
        "docstring_tokens = remove_after_dot(docstring_tokens)\n",
        "docstring_tokens = remove_non_ascii(docstring_tokens)\n",
        "docstring_tokens = remove_special(docstring_tokens)\n",
        "docstring_tokens = seperate_strings(docstring_tokens)\n",
        "docstring_tokens = remove_empty(docstring_tokens)\n",
        "docstring_tokens = fill_empty(identifier, docstring_tokens)\n",
        "docstring_tokens = lowercase(docstring_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqYmnNJDEM1W",
        "colab_type": "text"
      },
      "source": [
        "Applying preprocressing to the function tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkhqrkLI-CZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copying docstring_tokens column.\n",
        "function_tokens = data['function_tokens'].copy(deep=True)\n",
        "\n",
        "function_tokens = remove_non_ascii(function_tokens)\n",
        "function_tokens = seperate_strings(function_tokens)\n",
        "function_tokens = remove_comments(function_tokens)\n",
        "function_tokens = remove_empty(function_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdQE_s0x7zCZ",
        "colab_type": "text"
      },
      "source": [
        "Creating a DataFrame that consists of docstring tokens, functions and function tokens, and exporting it in pickle format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JGwrRmATmlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.concat([docstring_tokens, function_tokens], axis=1)\n",
        "dataset.to_pickle('/content/drive/My Drive/dataset.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}