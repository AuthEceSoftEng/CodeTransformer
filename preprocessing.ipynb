{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9nNxef-3KOh",
        "colab_type": "text"
      },
      "source": [
        "Installing the wget package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_twe3Df3ADs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfrQVi303Muy",
        "colab_type": "text"
      },
      "source": [
        "Importing useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyfE-Lwe3NDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import wget\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wc8h1gJ3RJW",
        "colab_type": "text"
      },
      "source": [
        "Downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUf5iSIO3Rci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip'\n",
        "wget.download(url, '/content/dataset.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UylxGZPt3VHc",
        "colab_type": "text"
      },
      "source": [
        "Unzipping the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFcvSMNM3VbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/dataset.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiQJQ7sA3YlC",
        "colab_type": "text"
      },
      "source": [
        "Unpickling the dataset and creating a DataFrame with its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItKw0XZl3ZGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pickle.load(open('java_dedupe_definitions_v2.pkl', 'rb'))\n",
        "data = pd.DataFrame(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzDYzUql3cU1",
        "colab_type": "text"
      },
      "source": [
        "Removing data without a docstring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMfUYSJ03cnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing rows with no docstrings.\n",
        "data = data[data['docstring_tokens'].map(lambda d: len(d)) > 0]\n",
        "# resetting DataFrame indices.\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# selecting only the first 50,000 rows for faster testing.\n",
        "#data = data[:50000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y-pnzJq3fw_",
        "colab_type": "text"
      },
      "source": [
        "Defining the preprocessing functions.\n",
        "\n",
        "*   *remove_after_dot*: removes all strings after the occurence of the first dot in the doscstring.\n",
        "*   *remove_non_ascii*: replaces all non-ASCII characters with an empty string.\n",
        "*   *remove_special*: replaces all special characters in the docstring with an empty string.\n",
        "*   *seperate_strings*: seperates strings that have at least one uppercase and one lowercase letter.\n",
        "*   *remove_empty*: removes all empty strings.\n",
        "*   *fill_empty*: empties docstrings with less than three or more than 30 words and fills them with words from function's identifier to perform data augmentation.\n",
        "*   *lowercase*: lowercases all strings in the docstring to avoid case sensitivity.\n",
        "*   *remove_unnecessary*: removes all string values and comments in the function.\n",
        "*   *trim*: keeps a maximum of 100 function tokens for each function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8na5Dwed3gCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_after_dot(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      if token == '.':\n",
        "        token_index = row.index(token)\n",
        "        data[index] = row[:token_index]\n",
        "        break\n",
        "  \n",
        "  return data\n",
        "\n",
        "def remove_non_ascii(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing non-ASCII characters with an empty string.\n",
        "      token = re.sub(r'[^\\x00-\\x7f]', '', token)\n",
        "      data[index][token_index] = token\n",
        "      \n",
        "  return data\n",
        "\n",
        "def remove_special(data, function=False):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def seperate_strings(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      # if the string has at least one uppercase and one lowercase letter.\n",
        "      if re.findall(r'[A-Z][a-z][^A-Z]*', token):\n",
        "        token_index = row.index(token)\n",
        "        # capitalizing the first letter of the token.\n",
        "        token = token[0].capitalize() + token[1:]\n",
        "        token = re.findall(r'[A-Z][a-z][^A-Z]*', token)\n",
        "        # replacing token with an empty string.\n",
        "        data[index][token_index] = ''\n",
        "        # adding the seperated words to the list preserving their original position.\n",
        "        data[index] = data[index][:token_index] + token + data[index][token_index:]\n",
        "        # updating row.\n",
        "        row = data[index]\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_empty(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      if not token:  \n",
        "        # removing empty strings from the list.\n",
        "        data[index] = list(filter(None, row))\n",
        "\n",
        "  return data\n",
        "\n",
        "def fill_empty(identifier, data):\n",
        "  for (index, row), identifier_row in zip(data.iteritems(), identifier):\n",
        "    if len(row) < 3 or len(row) > 30:\n",
        "        data[index] = []\n",
        "    if not data[index]:\n",
        "      # splitting identifiers on the dots.\n",
        "      augmented_row = identifier_row.split('.')\n",
        "      # capitalizing the first letter of the second half of the identifier.\n",
        "      augmented_row[1] = augmented_row[1][0].capitalize() + augmented_row[1][1:]\n",
        "      # seperating all identifier words using their first capital letter.\n",
        "      a = re.findall(r'[A-Z][^A-Z]*', augmented_row[0])\n",
        "      b = re.findall(r'[A-Z][^A-Z]*', augmented_row[1])\n",
        "      data[index] = a + b\n",
        "\n",
        "  return data\n",
        "\n",
        "def lowercase(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      token = token.lower()\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_unnecessary(data):\n",
        "  for index, row in data.iteritems():\n",
        "    for token in row:\n",
        "      # if the string contains space, double quotes or is a comment.\n",
        "      if re.findall(r'[ ]', token) or re.findall(r'(\")', token) or re.findall(r'(^//)', token) or re.findall(r'(^/\\*)', token) or re.findall(r'(^/\\*\\*)', token):\n",
        "        token_index = row.index(token)\n",
        "        # replacing token with an empty string.\n",
        "        data[index][token_index] = ''\n",
        "  \n",
        "  return data\n",
        "\n",
        "def trim(data):\n",
        "  for index, row in data.iteritems():\n",
        "    if len(row) > 100:\n",
        "      data[index] = row[:100]\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rvfphux3kkP",
        "colab_type": "text"
      },
      "source": [
        "Applying preprocressing to the docstring tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4xeRMoB3omj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copying docstring_tokens column.\n",
        "docstring_tokens = data['docstring_tokens'].copy(deep=True)\n",
        "# copying identifier column.\n",
        "identifier = data['identifier'].copy(deep=True)\n",
        "\n",
        "# applying the preprocessing functions on all docstring tokens.\n",
        "docstring_tokens = remove_after_dot(docstring_tokens)\n",
        "docstring_tokens = remove_non_ascii(docstring_tokens)\n",
        "docstring_tokens = remove_special(docstring_tokens)\n",
        "docstring_tokens = seperate_strings(docstring_tokens)\n",
        "docstring_tokens = remove_empty(docstring_tokens)\n",
        "docstring_tokens = fill_empty(identifier, docstring_tokens)\n",
        "docstring_tokens = lowercase(docstring_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mJNWUgA3tLs",
        "colab_type": "text"
      },
      "source": [
        "Applying preprocressing to the function tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSESsM2W3suu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copying docstring_tokens column.\n",
        "function_tokens = data['function_tokens'].copy(deep=True)\n",
        "\n",
        "function_tokens = remove_non_ascii(function_tokens)\n",
        "function_tokens = seperate_strings(function_tokens)\n",
        "function_tokens = remove_unnecessary(function_tokens)\n",
        "function_tokens = remove_special(function_tokens)\n",
        "function_tokens = remove_empty(function_tokens)\n",
        "function_tokens = trim(function_tokens)\n",
        "function_tokens = lowercase(function_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFM3s_LC3vMM",
        "colab_type": "text"
      },
      "source": [
        "Creating a DataFrame that consists of docstring tokens, functions and function tokens, and exporting it in pickle format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V21wGz3W3w06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.concat([docstring_tokens, function_tokens], axis=1)\n",
        "dataset.to_pickle('/content/drive/My Drive/dataset.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFsDQ8K43yJj",
        "colab_type": "text"
      },
      "source": [
        "Creating the docstring and function vocabularies, and exporting them in pickle format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQC-1SWU3z0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docstring_vocab = list(set(token for row in docstring_tokens for token in row))\n",
        "function_vocab = list(set(token for row in function_tokens for token in row))\n",
        "\n",
        "with open('/content/drive/My Drive/docstring_vocab.pkl', 'wb') as docstring_vocab_pkl:\n",
        "    pickle.dump(docstring_vocab, docstring_vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/function_vocab.pkl', 'wb') as function_vocab_pkl:\n",
        "    pickle.dump(function_vocab, function_vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}