{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNifeFm7vVo",
        "colab_type": "text"
      },
      "source": [
        "Installing the wget package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGYO-apCYXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rZDo54s7vqv",
        "colab_type": "text"
      },
      "source": [
        "Importing useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owo6okSwBFtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import wget\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "# disabling pandas' chain warnings.\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2oKoGnS7wK3",
        "colab_type": "text"
      },
      "source": [
        "Downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCToFP_NBtbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip'\n",
        "wget.download(url, '/content/dataset.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hl4n5mc7wxu",
        "colab_type": "text"
      },
      "source": [
        "Unzipping the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvXF3LTbCt3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/dataset.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mVkkAVB7xSR",
        "colab_type": "text"
      },
      "source": [
        "Unpickling the dataset and creating a DataFrame with its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduRE7TVSnQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pickle.load(open('java_dedupe_definitions_v2.pkl', 'rb'))\n",
        "data = pd.DataFrame(data)\n",
        "# selecting only the first 50,000 rows for faster testing.\n",
        "data = data[:50000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWRBc2Py7x1-",
        "colab_type": "text"
      },
      "source": [
        "Defining all preprocessing functions.\n",
        "\n",
        "*   *remove_after_dot*: removes all strings after the occurence of the first dot in the doscstring.\n",
        "*   *remove_non_ascii*: removes all non-ascii characters in the docstring.\n",
        "*   *remove_special*: removes all special characters in the docstring.\n",
        "*   *fill_empty*: fills empty docstrings with words from each function's identifier in order to perform data augmentation.\n",
        "*   *lowercase*: lowercases all strings in the docstring to avoid case sensitivity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oUIVlg3363f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_after_dot(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      if token == '.':\n",
        "        token_index = row.index(token)\n",
        "        docstring[index] = row[:token_index]\n",
        "        break\n",
        "  \n",
        "  return docstring\n",
        "\n",
        "def remove_non_ascii(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    regex = re.compile(r'[^\\x00-\\x7f]')\n",
        "    filtered_row = [i for i in row if not regex.match(i)]\n",
        "    docstring[index] = filtered_row\n",
        "\n",
        "  return docstring\n",
        "\n",
        "def remove_special(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub('[^A-Za-z0-9]+', '', token)\n",
        "      docstring[index][token_index] = token\n",
        "      if not token:\n",
        "        # popping empty strings from the list.\n",
        "        docstring[index].pop(token_index)\n",
        "\n",
        "  return docstring\n",
        "\n",
        "def fill_empty(identifier, docstring):\n",
        "  for (index, row), identifier_row in zip(docstring.iteritems(), identifier):\n",
        "    if not row:\n",
        "      # splitting identifiers on the dots.\n",
        "      augmented_row = identifier_row.split('.')\n",
        "      # capitalizing the first letter of the second half of the identifier.\n",
        "      augmented_row[1] = augmented_row[1].capitalize()\n",
        "      # seperating all identifier words using their first capital letter.\n",
        "      a = re.findall('[A-Z][^A-Z]*', augmented_row[0])\n",
        "      b = re.findall('[A-Z][^A-Z]*', augmented_row[1])\n",
        "      docstring[index] = a + b\n",
        "\n",
        "  return docstring\n",
        "\n",
        "def lowercase(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      token = token.lower()\n",
        "      docstring[index][token_index] = token\n",
        "\n",
        "  return docstring "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RenAKRKM7ypZ",
        "colab_type": "text"
      },
      "source": [
        "Applying preprocressing on all docstring tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPFG6PIUUWu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docstring_tokens = data['docstring_tokens']\n",
        "identifier = data['identifier']\n",
        "\n",
        "docstring_tokens = remove_after_dot(docstring_tokens)\n",
        "docstring_tokens = remove_non_ascii(docstring_tokens)\n",
        "docstring_tokens = remove_special(docstring_tokens)\n",
        "docstring_tokens = fill_empty(identifier, docstring_tokens)\n",
        "docstring_tokens = lowercase(docstring_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdQE_s0x7zCZ",
        "colab_type": "text"
      },
      "source": [
        "Creating a DataFrame that consists of docstring tokens, functions and function tokens. Also, creating a CSV for network usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JGwrRmATmlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.concat([docstring_tokens, data.function, data.function_tokens], axis=1)\n",
        "dataset.to_csv('/content/drive/My Drive/dataset.csv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}