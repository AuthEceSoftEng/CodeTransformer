{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding_matrices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wmEsxN6D_xL",
        "colab_type": "text"
      },
      "source": [
        "Importing useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8C7AXURC5uP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCYdKRm-EGDI",
        "colab_type": "text"
      },
      "source": [
        "Loading the dataset to a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhPdjHe1Dasf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset.pkl')\n",
        "\n",
        "# copying docstring_tokens column.\n",
        "docstring_tokens = dataset['docstring_tokens'].copy(deep=True)\n",
        "# copying function_tokens column.\n",
        "function_tokens = dataset['function_tokens'].copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HG-OaO8Eac3",
        "colab_type": "text"
      },
      "source": [
        "Building the docstring vocabulary and training with it a Word2Vec and a FastText model for 10 epochs. The embedding vector dimensions are set to 512."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qamdam1HIAKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word2Vec model.\n",
        "w2v = Word2Vec(size=512, min_count=0)\n",
        "w2v.build_vocab(docstring_tokens)\n",
        "w2v.train(docstring_tokens, total_examples=len(docstring_tokens), epochs=10)\n",
        "w2v.save('/content/drive/My Drive/w2v_model')\n",
        "\n",
        "# FastText model.\n",
        "ft = FastText(size=512, min_count=0)\n",
        "ft.build_vocab(docstring_tokens)\n",
        "ft.train(docstring_tokens, total_examples=len(docstring_tokens), epochs=10)\n",
        "ft.save('/content/drive/My Drive/ft_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPcnudekwpqc",
        "colab_type": "text"
      },
      "source": [
        "Building the function vocabulary and training with it a Doc2Vec model for 10 epochs. The embedding vector dimensions are set to 512."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhD33ZBPwwMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building the function token vocabulary.\n",
        "tagged_data = [TaggedDocument(function, [index]) for index, function in enumerate(function_tokens)]\n",
        "\n",
        "# Doc2Vec model.\n",
        "d2v = Doc2Vec(vector_size=512, min_count=0, workers=4)\n",
        "d2v.build_vocab(tagged_data)\n",
        "d2v.train(tagged_data, total_examples=len(tagged_data), epochs=10)\n",
        "d2v.save('/content/drive/My Drive/d2v_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EG1LW5QE5za",
        "colab_type": "text"
      },
      "source": [
        "Defining a function that creates embedding matrices. Each row contains the embedding vector of the corresponding word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLh3dKLZo81X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_matrix(embedding, vocabulary, doc2vec=False):\n",
        "  vocab_size = len(vocabulary)\n",
        "  # initializing the weight matrix.\n",
        "  embedding_matrix = np.zeros((vocab_size, 512))\n",
        "\n",
        "  if doc2vec:\n",
        "    for index, function in enumerate(vocabulary):\n",
        "      embedding_matrix[index] = embedding.infer_vector(function)\n",
        "  if not doc2vec:\n",
        "    for index, word in enumerate(vocabulary):\n",
        "      embedding_matrix[index] = embedding.wv[word]\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA2DmjGRF9Bv",
        "colab_type": "text"
      },
      "source": [
        "Creating the embedding matrices for each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxZAz1pMrKx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# storing the docstring vocabulary in a list.\n",
        "docstring_vocab = list(w2v.wv.vocab.keys())\n",
        "# storing the function vocabulary in a list.\n",
        "function_vocab = list(d2v.wv.vocab.keys())\n",
        "\n",
        "# Word2Vec embedding matrix.\n",
        "w2v_matrix = embedding_matrix(w2v, docstring_vocab)\n",
        "# FastText embedding matrix.\n",
        "ft_matrix = embedding_matrix(ft, docstring_vocab)\n",
        "# Doc2Vec embedding matrix.\n",
        "d2v_matrix = embedding_matrix(d2v, function_tokens, doc2vec=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Osz9sBGQkG",
        "colab_type": "text"
      },
      "source": [
        "Exporting the docstring vocabulary and the embedding matrices in pickle format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryKqnUxGylr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/docstring_vocab.pkl', 'wb') as docstring_vocab_pkl:\n",
        "    pickle.dump(docstring_vocab, docstring_vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/function_vocab.pkl', 'wb') as function_vocab_pkl:\n",
        "    pickle.dump(function_vocab, function_vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/w2v_matrix.pkl', 'wb') as w2v_pkl:\n",
        "    pickle.dump(w2v_matrix, w2v_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/ft_matrix.pkl', 'wb') as ft_pkl:\n",
        "    pickle.dump(ft_matrix, ft_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "with open('/content/drive/My Drive/d2v_matrix.pkl', 'wb') as d2v_pkl:\n",
        "    pickle.dump(d2v_matrix, d2v_pkl, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}