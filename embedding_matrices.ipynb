{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embedding_matrices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wmEsxN6D_xL",
        "colab_type": "text"
      },
      "source": [
        "Importing useful libraries and APIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8C7AXURC5uP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from gensim.models import Word2Vec, FastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCYdKRm-EGDI",
        "colab_type": "text"
      },
      "source": [
        "Loading the dataset to a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhPdjHe1Dasf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset.pkl')\n",
        "\n",
        "# copying docstring_tokens column.\n",
        "docstring_tokens = dataset['docstring_tokens'].copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HG-OaO8Eac3",
        "colab_type": "text"
      },
      "source": [
        "Building vocabulary and training with it a Word2Vec and a FastText model for 10 epochs. The embedding vector dimensions are set to 512."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qamdam1HIAKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word2Vec model.\n",
        "w2v = Word2Vec(size=512, min_count=0)\n",
        "w2v.build_vocab(docstring_tokens)\n",
        "w2v.train(docstring_tokens, total_examples=len(docstring_tokens), epochs=10)\n",
        "\n",
        "# FastText model.\n",
        "ft = FastText(size=512, min_count=0)\n",
        "ft.build_vocab(docstring_tokens)\n",
        "ft.train(docstring_tokens, total_examples=len(docstring_tokens), epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EG1LW5QE5za",
        "colab_type": "text"
      },
      "source": [
        "Defining a function that creates embedding matrices. Each row contains the embedding vector of the corresponding word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLh3dKLZo81X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_matrix(embedding, vocabulary):\n",
        "  vocab_size = len(vocab_list)\n",
        "  # initializing the weight matrix.\n",
        "  embedding_matrix = np.zeros((vocab_size, 512))\n",
        "\n",
        "  for index, word in enumerate(vocab_list):\n",
        "    embedding_matrix[index] = embedding.wv[word]\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA2DmjGRF9Bv",
        "colab_type": "text"
      },
      "source": [
        "Creating the embedding matrices for each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxZAz1pMrKx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# storing the vocabulary in a list.\n",
        "vocab_list = list(w2v.wv.vocab.keys())\n",
        "\n",
        "# Word2Vec embedding matrix.\n",
        "w2v_matrix = embedding_matrix(w2v, vocab_list)\n",
        "\n",
        "# FastText embedding matrix.\n",
        "ft_matrix = embedding_matrix(ft, vocab_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Osz9sBGQkG",
        "colab_type": "text"
      },
      "source": [
        "Exporting the vocabulary and the embedding matrices in pickle format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OryKqnUxGylr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/vocabulary.pkl', 'wb') as vocab_pkl:\n",
        "    pickle.dump(vocab_list, vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/w2v_matrix.pkl', 'wb') as w2v_pkl:\n",
        "    pickle.dump(w2v_matrix, w2v_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "with open('/content/drive/My Drive/ft_matrix.pkl', 'wb') as ft_pkl:\n",
        "    pickle.dump(ft_matrix, ft_pkl, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}