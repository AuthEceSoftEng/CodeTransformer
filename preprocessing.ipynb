{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd-jbWL3DUpq",
        "colab_type": "text"
      },
      "source": [
        "OPEN IN PLAYGROUND MODE: https://colab.research.google.com/drive/1XeqqZxR33AiuFh_7z1lzrP_uf9RFhdft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsNifeFm7vVo",
        "colab_type": "text"
      },
      "source": [
        "Installing the wget package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGYO-apCYXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rZDo54s7vqv",
        "colab_type": "text"
      },
      "source": [
        "Importing useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owo6okSwBFtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import wget\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2oKoGnS7wK3",
        "colab_type": "text"
      },
      "source": [
        "Downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCToFP_NBtbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip'\n",
        "wget.download(url, '/content/dataset.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hl4n5mc7wxu",
        "colab_type": "text"
      },
      "source": [
        "Unzipping the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvXF3LTbCt3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/dataset.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mVkkAVB7xSR",
        "colab_type": "text"
      },
      "source": [
        "Unpickling the dataset and creating a DataFrame with its data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduRE7TVSnQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pickle.load(open('java_dedupe_definitions_v2.pkl', 'rb'))\n",
        "data = pd.DataFrame(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWRBc2Py7x1-",
        "colab_type": "text"
      },
      "source": [
        "Defining all preprocessing functions.\n",
        "\n",
        "*   *remove_after_dot*: removes all strings after the occurence of the first dot in the doscstring.\n",
        "*   *remove_non_ascii*: replaces all non-ascii characters in the docstring with an empty string.\n",
        "*   *remove_special*: replaces all special characters in the docstring with an empty string.\n",
        "*   *remove_empty*: removes all empty strings in the docstring.\n",
        "*   *fill_empty*: empties docstrings with less than three words and fills them with words from function's identifier in order to perform data augmentation.\n",
        "*   *lowercase*: lowercases all strings in the docstring to avoid case sensitivity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oUIVlg3363f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_after_dot(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      if token == '.':\n",
        "        token_index = row.index(token)\n",
        "        docstring[index] = row[:token_index]\n",
        "        break\n",
        "  \n",
        "  return docstring\n",
        "\n",
        "def remove_non_ascii(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^\\x00-\\x7f]', '', token)\n",
        "      docstring[index][token_index] = token\n",
        "      \n",
        "  return docstring\n",
        "\n",
        "def remove_special(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n",
        "      docstring[index][token_index] = token\n",
        "\n",
        "  return docstring\n",
        "\n",
        "def remove_empty(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      if not token:  \n",
        "        # removing empty strings from the list.\n",
        "        docstring[index] = list(filter(None, row))\n",
        "\n",
        "  return docstring\n",
        "\n",
        "def fill_empty(identifier, docstring):\n",
        "  for (index, row), identifier_row in zip(docstring.iteritems(), identifier):\n",
        "    if len(row) < 3:\n",
        "        docstring[index] = []\n",
        "    if not docstring[index]:\n",
        "      # splitting identifiers on the dots.\n",
        "      augmented_row = identifier_row.split('.')\n",
        "      # capitalizing the first letter of the second half of the identifier.\n",
        "      augmented_row[1] = augmented_row[1].capitalize()\n",
        "      # seperating all identifier words using their first capital letter.\n",
        "      a = re.findall(r'[A-Z][^A-Z]*', augmented_row[0])\n",
        "      b = re.findall(r'[A-Z][^A-Z]*', augmented_row[1])\n",
        "      docstring[index] = a + b\n",
        "\n",
        "  return docstring\n",
        "\n",
        "def lowercase(docstring):\n",
        "  for index, row in docstring.iteritems():\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      token = token.lower()\n",
        "      docstring[index][token_index] = token\n",
        "\n",
        "  return docstring"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RenAKRKM7ypZ",
        "colab_type": "text"
      },
      "source": [
        "Applying preprocressing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPFG6PIUUWu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing rows with no docstrings.\n",
        "data = data[data['docstring_tokens'].map(lambda d: len(d)) > 0]\n",
        "# resetting DataFrame indices.\n",
        "data = data.reset_index(drop=True)\n",
        "\n",
        "# selecting only the first 50,000 rows for faster testing.\n",
        "data = data[:50000]\n",
        "\n",
        "# copying docstring_tokens column.\n",
        "docstring_tokens = data['docstring_tokens'].copy(deep=True)\n",
        "# copying identifier column.\n",
        "identifier = data['identifier'].copy(deep=True)\n",
        "\n",
        "# applying the preprocessing functions on all docstring tokens.\n",
        "docstring_tokens = remove_after_dot(docstring_tokens)\n",
        "docstring_tokens = remove_non_ascii(docstring_tokens)\n",
        "docstring_tokens = remove_special(docstring_tokens)\n",
        "docstring_tokens = remove_empty(docstring_tokens)\n",
        "docstring_tokens = fill_empty(identifier, docstring_tokens)\n",
        "docstring_tokens = lowercase(docstring_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdQE_s0x7zCZ",
        "colab_type": "text"
      },
      "source": [
        "Creating a DataFrame that consists of docstring tokens, functions and function tokens. Also, creating a CSV for network usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JGwrRmATmlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.concat([docstring_tokens, data.function, data.function_tokens], axis=1)\n",
        "dataset.to_csv('/content/drive/My Drive/dataset.csv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}