{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1wiewKMyeuhqB_RVd9ZRHYWEbNOeCKOKT","authorship_tag":"ABX9TyPk0lJOVP5MwORVoJKmtuuM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"p9nNxef-3KOh","colab_type":"text"},"source":["Installing the wget package."]},{"cell_type":"code","metadata":{"id":"x_twe3Df3ADs","colab_type":"code","colab":{}},"source":["pip install wget"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bfrQVi303Muy","colab_type":"text"},"source":["Importing libraries and APIs."]},{"cell_type":"code","metadata":{"id":"vyfE-Lwe3NDd","colab_type":"code","colab":{}},"source":["import re\n","import wget\n","import gzip\n","import shutil\n","import pickle\n","import collections\n","import pandas as pd\n","import numpy as np\n","import tensorflow_datasets as tfds"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2wc8h1gJ3RJW","colab_type":"text"},"source":["Downloading the dataset."]},{"cell_type":"code","metadata":{"id":"cUf5iSIO3Rci","colab_type":"code","colab":{}},"source":["url = 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/java.zip'\n","wget.download(url, '/content/dataset.zip')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UylxGZPt3VHc","colab_type":"text"},"source":["Unzipping the dataset."]},{"cell_type":"code","metadata":{"id":"WFcvSMNM3VbK","colab_type":"code","colab":{}},"source":["!unzip '/content/dataset.zip'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKhZstV9VF88","colab_type":"text"},"source":["Unzipping the training, validation and test datasets and creating dataFrames with their data."]},{"cell_type":"code","metadata":{"id":"-4Ay5o8l8O3Y","colab_type":"code","colab":{}},"source":["path = '/content/java/final/jsonl/MODE/java_MODE_INDEX.jsonl'\n","mode = ['train', 'valid', 'test']\n","\n","for m in mode:\n","  if m == 'train':\n","    train_data = pd.DataFrame()\n","\n","    for i in range(0, 16):\n","      file_path = path.replace('MODE', m)\n","      file_path = file_path.replace('INDEX', str(i))\n","\n","      with gzip.open(file_path + '.gz', 'rb') as f_in:\n","        with open(file_path, 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","\n","      train_data_temp = pd.read_json(file_path, lines=True)\n","      train_data = train_data.append(train_data_temp)\n","\n","    # resetting indices.\n","    train_data = train_data.reset_index(drop=True)\n","\n","  elif m == 'valid':\n","    valid_data = pd.DataFrame()\n","\n","    for i in range(0, 1):\n","      file_path = path.replace('MODE', m)\n","      file_path = file_path.replace('INDEX', str(i))\n","\n","      with gzip.open(file_path + '.gz', 'rb') as f_in:\n","        with open(file_path, 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","\n","      valid_data_temp = pd.read_json(file_path, lines=True)\n","      valid_data = valid_data.append(valid_data_temp)\n","\n","  elif m == 'test':\n","    test_data = pd.DataFrame()\n","\n","    for i in range(0, 1):\n","      file_path = path.replace('MODE', m)\n","      file_path = file_path.replace('INDEX', str(i))\n","\n","      with gzip.open(file_path + '.gz', 'rb') as f_in:\n","        with open(file_path, 'wb') as f_out:\n","            shutil.copyfileobj(f_in, f_out)\n","\n","      test_data_temp = pd.read_json(file_path, lines=True)\n","      test_data = test_data.append(test_data_temp)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UiQJQ7sA3YlC","colab_type":"text"},"source":["Unpickling the whole corpus and creating a DataFrame with its data."]},{"cell_type":"code","metadata":{"id":"ItKw0XZl3ZGN","colab_type":"code","colab":{}},"source":["corpus = pickle.load(open('java_dedupe_definitions_v2.pkl', 'rb'))\n","corpus = pd.DataFrame(corpus)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2y-pnzJq3fw_","colab_type":"text"},"source":["Defining the preprocessing functions.\n","\n","*   *remove_after_dot*: removes all strings after the occurence of the first dot in the doscstring.\n","*   *remove_non_ascii*: replaces all non-ASCII characters with an empty string.\n","*   *remove_special*: replaces all special characters in the docstring with an empty string.\n","*   *seperate_strings*: seperates camelCase strings.\n","*   *remove_empty*: removes all empty strings.\n","*   *fill_empty*: empties docstrings with less than 6 or more than 30 words and fills them with words from function's name to perform data augmentation.\n","*   *lowercase*: lowercases all strings in the docstring to avoid case sensitivity.\n","*   *remove_unnecessary*: removes all string values and comments in the code.\n","*   *replace_symbols*: replaces specific programming symbols with their names.\n","*   *trim*: keeps a maximum of 100 code tokens for each code."]},{"cell_type":"code","metadata":{"id":"8na5Dwed3gCe","colab_type":"code","colab":{}},"source":["def remove_after_dot(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      if token == '.':\n","        token_index = row.index(token)\n","        data[index] = row[:token_index]\n","        break\n","  \n","  return data\n","\n","def remove_non_ascii(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      token_index = row.index(token)\n","      # replacing non-ASCII characters with an empty string.\n","      token = re.sub(r'[^\\x00-\\x7f]', '', token)\n","      data[index][token_index] = token\n","      \n","  return data\n","\n","def remove_special(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      token_index = row.index(token)\n","      # replacing special characters with an empty string.\n","      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n","      data[index][token_index] = token\n","\n","  return data\n","\n","def seperate_strings(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      # if the string is in camelCase format.\n","      if re.findall(r'[A-Z][a-z][^A-Z]*', token):\n","        token_index = row.index(token)\n","        # capitalizing the first letter of the token.\n","        token = token[0].capitalize() + token[1:]\n","        token = re.findall(r'[A-Z][a-z][^A-Z]*|[A-Z]*(?![a-z])|[A-Z][a-z][^A-Z]*', token)\n","        # replacing token with an empty string.\n","        data[index][token_index] = ''\n","        # adding the seperated words to the list preserving their original position.\n","        data[index] = data[index][:token_index] + token + data[index][token_index:]\n","        # updating row.\n","        row = data[index]\n","\n","  return data\n","\n","def remove_empty(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      if not token:  \n","        # removing empty strings from the list.\n","        data[index] = list(filter(None, row))\n","\n","  return data\n","\n","def fill_empty(function_name, data):\n","  for (index, row), function_name_row in zip(data.iteritems(), function_name):\n","    if len(row) < 6 or len(row) > 30:\n","        data[index] = []\n","    if not data[index]:\n","      # splitting function's name on the dots.\n","      augmented_row = function_name_row.split('.')\n","      # capitalizing the first letter of the second half of the function's name.\n","      augmented_row[1] = augmented_row[1][0].capitalize() + augmented_row[1][1:]\n","      # seperating all function's name words of the second half using their first capital letter.\n","      data[index] = re.findall(r'[A-Z][a-z][^A-Z]*|[A-Z]*(?![a-z])|[A-Z][a-z][^A-Z]*', augmented_row[1])\n","\n","  return data\n","\n","def lowercase(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      token_index = row.index(token)\n","      token = token.lower()\n","      data[index][token_index] = token\n","\n","  return data\n","\n","def remove_unnecessary(data):\n","  for index, row in data.iteritems():\n","    for token in row:\n","      # if the string contains space, double quotes or is a comment.\n","      if re.findall(r'[ ]', token) or re.findall(r'(\")', token) or re.findall(r'(^//)', token) or re.findall(r'(^/\\*)', token) or re.findall(r'(^/\\*\\*)', token):\n","        token_index = row.index(token)\n","        # replacing token with an empty string.\n","        data[index][token_index] = ''\n","  \n","  return data\n","\n","def replace_symbols(data):\n","  dictionary = {'(': 'openingparenthesis', \n","                ')': 'closingparenthesis',\n","                '[': 'openingbracket', \n","                ']': 'closingbracket',\n","                '{': 'openingbrace', \n","                '}': 'closingbrace',\n","                '+': 'addoperator', \n","                '-': 'subtractoperator',\n","                '*': 'multiplyoperator', \n","                '/': 'divideoperator',\n","                '^': 'poweroperator', \n","                '%': 'modulooperator',\n","                '=': 'assignoperator', \n","                '==': 'equaloperator',\n","                '!=': 'notequaloperator', \n","                '>': 'greateroperator',\n","                '<': 'lessoperator', \n","                '>=': 'greaterequaloperator',\n","                '<=': 'lessequaloperator', \n","                '++': 'incrementoperator',\n","                '--': 'decrementoperator', \n","                '!': 'notoperator',\n","                '@': 'atsign',\n","                ';': 'semicolon'}\n","\n","  for index, row in data.iteritems():\n","    for token in row:\n","      # if the string contains one or more of the following symbols.\n","      if re.findall(r'^[()[\\]{}<>+\\-*/^%=!@;]', token):\n","        token_index = row.index(token)\n","        # replacing token with the name of the symbol contained.\n","        for symbol, name in dictionary.items():\n","          if token == symbol:\n","            data[index][token_index] = name\n","        \n","  return data\n","\n","def trim(data):\n","  for index, row in data.iteritems():\n","    if len(row) > 100:\n","      data[index] = row[:100]\n","\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Rvfphux3kkP","colab_type":"text"},"source":["Applying preprocressing to the docstring tokens of the training, validation and test datasets."]},{"cell_type":"code","metadata":{"id":"N4xeRMoB3omj","colab_type":"code","colab":{}},"source":["# copying docstring_tokens column of the training dataset.\n","train_docstring_tokens = train_data['docstring_tokens'].copy(deep=True)\n","# copying func_name column of the training dataset.\n","train_function_name = train_data['func_name'].copy(deep=True)\n","\n","# applying the preprocessing functions on all docstring tokens of the training dataset.\n","train_docstring_tokens = remove_after_dot(train_docstring_tokens)\n","train_docstring_tokens = remove_non_ascii(train_docstring_tokens)\n","train_docstring_tokens = remove_special(train_docstring_tokens)\n","train_docstring_tokens = seperate_strings(train_docstring_tokens)\n","train_docstring_tokens = remove_empty(train_docstring_tokens)\n","train_docstring_tokens = fill_empty(train_function_name, train_docstring_tokens)\n","train_docstring_tokens = remove_empty(train_docstring_tokens)\n","train_docstring_tokens = lowercase(train_docstring_tokens)\n","\n","# copying docstring_tokens column of the validation dataset.\n","valid_docstring_tokens = valid_data['docstring_tokens'].copy(deep=True)\n","# copying func_name column of the validation dataset.\n","valid_function_name = valid_data['func_name'].copy(deep=True)\n","\n","# applying the preprocessing functions on all docstring tokens of the validation dataset.\n","valid_docstring_tokens = remove_after_dot(valid_docstring_tokens)\n","valid_docstring_tokens = remove_non_ascii(valid_docstring_tokens)\n","valid_docstring_tokens = remove_special(valid_docstring_tokens)\n","valid_docstring_tokens = seperate_strings(valid_docstring_tokens)\n","valid_docstring_tokens = remove_empty(valid_docstring_tokens)\n","valid_docstring_tokens = fill_empty(valid_function_name, valid_docstring_tokens)\n","valid_docstring_tokens = remove_empty(valid_docstring_tokens)\n","valid_docstring_tokens = lowercase(valid_docstring_tokens)\n","\n","# copying docstring_tokens column of the test dataset.\n","test_docstring_tokens = test_data['docstring_tokens'].copy(deep=True)\n","# copying func_name column of the test dataset.\n","test_function_name = test_data['func_name'].copy(deep=True)\n","\n","# applying the preprocessing functions on all docstring tokens of the test dataset.\n","test_docstring_tokens = remove_after_dot(test_docstring_tokens)\n","test_docstring_tokens = remove_non_ascii(test_docstring_tokens)\n","test_docstring_tokens = remove_special(test_docstring_tokens)\n","test_docstring_tokens = seperate_strings(test_docstring_tokens)\n","test_docstring_tokens = remove_empty(test_docstring_tokens)\n","test_docstring_tokens = fill_empty(test_function_name, test_docstring_tokens)\n","test_docstring_tokens = remove_empty(test_docstring_tokens)\n","test_docstring_tokens = lowercase(test_docstring_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mJNWUgA3tLs","colab_type":"text"},"source":["Applying preprocressing to the code tokens of the training, validation and test datasets."]},{"cell_type":"code","metadata":{"id":"DSESsM2W3suu","colab_type":"code","colab":{}},"source":["# copying code_tokens column of the training dataset.\n","train_code_tokens = train_data['code_tokens'].copy(deep=True)\n","\n","# applying the preprocessing functions on all code tokens of the training dataset.\n","train_code_tokens = remove_non_ascii(train_code_tokens)\n","train_code_tokens = seperate_strings(train_code_tokens)\n","train_code_tokens = remove_unnecessary(train_code_tokens)\n","train_code_tokens = replace_symbols(train_code_tokens)\n","train_code_tokens = remove_special(train_code_tokens)\n","train_code_tokens = remove_empty(train_code_tokens)\n","train_code_tokens = trim(train_code_tokens)\n","train_code_tokens = lowercase(train_code_tokens)\n","\n","# copying code_tokens column of the validation dataset.\n","valid_code_tokens = valid_data['code_tokens'].copy(deep=True)\n","\n","# applying the preprocessing functions on all code tokens of the validation dataset.\n","valid_code_tokens = remove_non_ascii(valid_code_tokens)\n","valid_code_tokens = seperate_strings(valid_code_tokens)\n","valid_code_tokens = remove_unnecessary(valid_code_tokens)\n","valid_code_tokens = replace_symbols(valid_code_tokens)\n","valid_code_tokens = remove_special(valid_code_tokens)\n","valid_code_tokens = remove_empty(valid_code_tokens)\n","valid_code_tokens = trim(valid_code_tokens)\n","valid_code_tokens = lowercase(valid_code_tokens)\n","\n","# copying code_tokens column of the test dataset.\n","test_code_tokens = test_data['code_tokens'].copy(deep=True)\n","\n","# applying the preprocessing functions on all code tokens of the test dataset.\n","test_code_tokens = remove_non_ascii(test_code_tokens)\n","test_code_tokens = seperate_strings(test_code_tokens)\n","test_code_tokens = remove_unnecessary(test_code_tokens)\n","test_code_tokens = replace_symbols(test_code_tokens)\n","test_code_tokens = remove_special(test_code_tokens)\n","test_code_tokens = remove_empty(test_code_tokens)\n","test_code_tokens = trim(test_code_tokens)\n","test_code_tokens = lowercase(test_code_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFM3s_LC3vMM","colab_type":"text"},"source":["Creating DataFrames that consist of docstring tokens and code tokens of the training, validation and test datasets, and exporting them in pickle format."]},{"cell_type":"code","metadata":{"id":"V21wGz3W3w06","colab_type":"code","colab":{}},"source":["train_dataset = pd.concat([train_docstring_tokens, train_code_tokens], axis=1)\n","train_dataset.to_pickle('/content/drive/My Drive/train_dataset.pkl')\n","\n","valid_dataset = pd.concat([valid_docstring_tokens, valid_code_tokens], axis=1)\n","valid_dataset.to_pickle('/content/drive/My Drive/valid_dataset.pkl')\n","\n","test_dataset = pd.concat([test_docstring_tokens, test_code_tokens], axis=1)\n","test_dataset.to_pickle('/content/drive/My Drive/test_dataset.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFsDQ8K43yJj","colab_type":"text"},"source":["Creating the docstring and code vocabularies, and exporting them in pickle format."]},{"cell_type":"code","metadata":{"id":"zQC-1SWU3z0k","colab_type":"code","colab":{}},"source":["# vocabulary of the 10,000 most common docstring tokens.\n","docstring_vocab = list(token for row in train_docstring_tokens for token in row)\n","docstring_vocab = collections.Counter(docstring_vocab)\n","docstring_vocab = dict(docstring_vocab.most_common(10000))\n","docstring_vocab = list(docstring_vocab.keys())\n","# vocabulary of the 10,000 most common code tokens.\n","code_vocab = list(token for row in train_code_tokens for token in row)\n","code_vocab = collections.Counter(code_vocab)\n","code_vocab = dict(code_vocab.most_common(10000))\n","code_vocab = list(code_vocab.keys())\n","\n","with open('/content/drive/My Drive/docstring_vocab.pkl', 'wb') as docstring_vocab_pkl:\n","    pickle.dump(docstring_vocab, docstring_vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('/content/drive/My Drive/code_vocab.pkl', 'wb') as code_vocab_pkl:\n","    pickle.dump(code_vocab, code_vocab_pkl, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_W6SuQILaHQ","colab_type":"text"},"source":["Applying preprocessing to the code tokens of the whole corpus."]},{"cell_type":"code","metadata":{"id":"crHDqfDJLZqx","colab_type":"code","colab":{}},"source":["# copying code_tokens column of the whole corpus.\n","corpus_function_tokens = corpus['function_tokens'].copy(deep=True)\n","\n","# applying the preprocessing functions on all function tokens of the whole corpus.\n","corpus_function_tokens = remove_non_ascii(corpus_function_tokens)\n","corpus_function_tokens = seperate_strings(corpus_function_tokens)\n","corpus_function_tokens = remove_unnecessary(corpus_function_tokens)\n","corpus_function_tokens = replace_symbols(corpus_function_tokens)\n","corpus_function_tokens = remove_special(corpus_function_tokens)\n","corpus_function_tokens = remove_empty(corpus_function_tokens)\n","corpus_function_tokens = trim(corpus_function_tokens)\n","corpus_function_tokens = lowercase(corpus_function_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dwiGdV-4Psu-","colab_type":"text"},"source":["Creating a DataFrame that consists of code, code tokens and URLs of the whole corpus, and exporting it in pickle format."]},{"cell_type":"code","metadata":{"id":"aucIM8D7PsJm","colab_type":"code","colab":{}},"source":["functions = pd.concat([corpus.functions, corpus_function_tokens, corpus.url], axis=1)\n","functions.to_pickle('/content/drive/My Drive/functions.pkl')"],"execution_count":0,"outputs":[]}]}