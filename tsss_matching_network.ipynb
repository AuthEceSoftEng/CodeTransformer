{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tsss_matching_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1YOCbKjr656",
        "colab_type": "text"
      },
      "source": [
        "Installing Spotify's Annoy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhB0iPM9S1sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install annoy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMISbex-r86K",
        "colab_type": "text"
      },
      "source": [
        "Installing the W&B package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHkMD5VkgWw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZQgL9jr_A8",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries and APIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSfzHqa6Sm4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "import math as m\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goVlbp64sA-_",
        "colab_type": "text"
      },
      "source": [
        "Initiating a run on W&B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBjMaOPGhqmb",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "wandb.init(project='self-attention-tsss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKiGbNu8sCiM",
        "colab_type": "text"
      },
      "source": [
        "Loading the TensorBoard notebook extension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaaMa8H-cN7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-fd2-E0sEF4",
        "colab_type": "text"
      },
      "source": [
        "Loading the dataset and the vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfF4zQM_SrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the dataset.\n",
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset.pkl')\n",
        "\n",
        "# copying docstring_tokens column.\n",
        "docstring_tokens = dataset['docstring_tokens'].copy(deep=True)\n",
        "# copying function_tokens column.\n",
        "function_tokens = dataset['function_tokens'].copy(deep=True)\n",
        "\n",
        "# loading the docstring vocabulary.\n",
        "docstring_vocab =  pickle.load(open('/content/drive/My Drive/docstring_vocab.pkl', 'rb'))\n",
        "# loading the function vocabulary.\n",
        "function_vocab =  pickle.load(open('/content/drive/My Drive/function_vocab.pkl', 'rb'))\n",
        "\n",
        "# loading the functions, function tokens and URLs of the whole corpus.\n",
        "functions = pd.read_pickle('/content/drive/My Drive/functions.pkl')\n",
        "\n",
        "# copying corpus' function_tokens column.\n",
        "corpus_function_tokens = functions['function_tokens'].copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kCaF4fsIWj",
        "colab_type": "text"
      },
      "source": [
        "Defining the encoding functions.\n",
        "\n",
        "*   *encode*: encodes all words in a dataset to integers.\n",
        "*   *to_numpy*: converts the dataframes to numpy and pads them with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELRc6XXkNyQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(inp, tar, input_encoder, target_encoder):\n",
        "  # encoding input data.\n",
        "  for index, row in enumerate(inp):\n",
        "    inp[index] = [input_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  # encoding target data.\n",
        "  for index, row in enumerate(tar):\n",
        "    tar[index] = [target_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  return inp, tar\n",
        "\n",
        "def to_numpy(inp, tar):\n",
        "  # converting input data to numpy.\n",
        "  inp = pd.DataFrame(list(inp))\n",
        "  inp = inp.to_numpy()\n",
        "  inp = np.nan_to_num(inp)\n",
        "  inp = inp.astype(int)\n",
        "\n",
        "  # converting target data to numpy.\n",
        "  tar = pd.DataFrame(list(tar))\n",
        "  tar = tar.to_numpy()\n",
        "  tar = np.nan_to_num(tar)\n",
        "  tar = tar.astype(int)\n",
        "\n",
        "  return inp, tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhcfZJSsHkW",
        "colab_type": "text"
      },
      "source": [
        "Splitting the dataset to training and validation sets and applying the encoding functions to the docstring and function tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLmqTMk4NzTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building input_encoder.\n",
        "input_encoder = tfds.features.text.TokenTextEncoder(docstring_vocab)\n",
        "# building target_encoder.\n",
        "target_encoder = tfds.features.text.TokenTextEncoder(function_vocab)\n",
        "\n",
        "# initializing original_input list of lists.\n",
        "original_input = docstring_tokens.copy(deep=True)\n",
        "# initializing original_target list.\n",
        "original_target = function_tokens.copy(deep=True)\n",
        "\n",
        "# splitting to training set.\n",
        "training_input = original_input[:480000]\n",
        "training_target = original_target[:480000]\n",
        "# splitting to validation set.\n",
        "validation_input = original_input[480000:499000]\n",
        "validation_input = validation_input.reset_index(drop=True)\n",
        "validation_target = original_target[480000:499000]\n",
        "validation_target = validation_target.reset_index(drop=True)\n",
        "\n",
        "# applying encoding to input and target data.\n",
        "encoded_training_input, encoded_training_target = encode(training_input, training_target, input_encoder, target_encoder)\n",
        "encoded_validation_input, encoded_validation_target = encode(validation_input, validation_target, input_encoder, target_encoder)\n",
        "# converting input and target data to numpy.\n",
        "encoded_training_input, encoded_training_target = to_numpy(encoded_training_input, encoded_training_target)\n",
        "encoded_validation_input, encoded_validation_target = to_numpy(encoded_validation_input, encoded_validation_target)\n",
        "\n",
        "# applying encoding to the whole corpus and converting it to numpy.\n",
        "_, encoded_corpus_target = encode([[]], corpus_function_tokens, input_encoder, target_encoder)\n",
        "_, encoded_corpus_target = to_numpy([[]], encoded_corpus_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rtWI9O2sN1N",
        "colab_type": "text"
      },
      "source": [
        "Creating the training and validation tensor datasets using Tensorflow Datasets API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3aKCL3mcM-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 500000\n",
        "\n",
        "# creating a tensor dataset with the training data.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((encoded_training_input, encoded_training_target))\n",
        "# caching the dataset for performance optimizations.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
        "train_dataset = train_dataset.batch(128)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# creating a tensor dataset with the validation data.\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((encoded_validation_input, encoded_validation_target))\n",
        "# caching the dataset for performance optimizations.\n",
        "valid_dataset = valid_dataset.cache()\n",
        "valid_dataset = valid_dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
        "valid_dataset = valid_dataset.batch(1000)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# creating a tensor dataset with the whole corpus.\n",
        "corpus_dataset = tf.data.Dataset.from_tensor_slices(encoded_corpus_target)\n",
        "# caching the dataset for performance optimizations.\n",
        "corpus_dataset = corpus_dataset.cache()\n",
        "corpus_dataset = corpus_dataset.batch(128)\n",
        "corpus_dataset = corpus_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF4J5F2MsPYv",
        "colab_type": "text"
      },
      "source": [
        "Creating the Positional Encoding layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5jo2SY2cbAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # creating the positional encoding matrix.\n",
        "    self.pe = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # storing word positions to a matrix.\n",
        "    position = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
        "    # storing embedding components to a matrix.\n",
        "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
        "\n",
        "    # calculating the angles.\n",
        "    angle = tf.multiply(position, 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, dtype=tf.float32)))\n",
        "\n",
        "    # applying sine to the angles of even indices.\n",
        "    sines = tf.sin(angle[:, 0::2])\n",
        "    # applying cosine to the angles of odd indices.\n",
        "    cosines = tf.cos(angle[:, 1::2])\n",
        "\n",
        "    # concatenating sines and cosines in one matrix.\n",
        "    pe = tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pe, dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    # adding positional encoding to the input embeddings on call.\n",
        "    return x + self.pe[:, :tf.shape(x)[-2], :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2O2WkqUsRw_",
        "colab_type": "text"
      },
      "source": [
        "Creating the Multi-Head Attention layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tTskI9ccaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, d_model):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    self.head_size = d_model // num_heads\n",
        "\n",
        "    # creating the weight matrices for each head.\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # creating the weight matrix for the output.\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, query, key, value, mask):\n",
        "    # storing the batch size.\n",
        "    batch_size = tf.shape(query)[-3]\n",
        "\n",
        "    # passing query, key and value as input to the weight matrices.\n",
        "    query = self.wq(query)\n",
        "    key = self.wk(key)\n",
        "    value = self.wv(value)\n",
        "\n",
        "    # splitting the dense tensors for each head.\n",
        "    query = tf.reshape(query, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    key = tf.reshape(key, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    value = tf.reshape(value, [batch_size, -1, self.num_heads, self.head_size])\n",
        "\n",
        "    # transposing the number of heads and sequence length columns.\n",
        "    query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "    key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "    value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # calculating the similarity score.\n",
        "    query_keyT = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # calculating the depth.\n",
        "    depth = tf.cast(tf.shape(key)[-1], dtype=tf.float32)\n",
        "    # calculating the scale factor.\n",
        "    scale = 1 / tf.sqrt(depth)\n",
        "\n",
        "    # calculating the scaled similarity scores.\n",
        "    scores = query_keyT * scale\n",
        "\n",
        "    # masking out key/value pairs.\n",
        "    if mask is not None:\n",
        "      scores *= mask\n",
        "      scores = tf.where(tf.equal(scores, 0), tf.ones_like(scores) * -1e9, scores)\n",
        "\n",
        "    # calculating the scaled similarity scores' softmax matrix.\n",
        "    softmax = tf.nn.softmax(scores)\n",
        "\n",
        "    # calculating the scaled dot-product attention for each head.\n",
        "    attention = tf.matmul(softmax, value)\n",
        "    attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "\n",
        "    # concatenating the attention heads.\n",
        "    output = tf.reshape(attention, [batch_size, -1, self.d_model])\n",
        "    # passing the concatenation as input to a dense layer.\n",
        "    output = self.dense(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCUbMKGIsUS0",
        "colab_type": "text"
      },
      "source": [
        "Creating the Feed Forward Network layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyoVUxxrcd41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dff, d_model):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "    # creating the dense layers of the feed forward network.\n",
        "    self.fc1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    # applying the layer with dff units and relu activation.\n",
        "    fc1 = self.fc1(x)\n",
        "    # applying the layer with d_model units and no activation.\n",
        "    output = self.fc2(fc1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyG-OV7XsVwP",
        "colab_type": "text"
      },
      "source": [
        "Creating the Encoder layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIACE_slcfAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, dff, d_model, rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    # creating the MHA and FFN layers.\n",
        "    self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "    self.ffn = FeedForwardNetwork(dff, d_model)\n",
        "\n",
        "    # creating the dropout layers.\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the normalization layers.\n",
        "    self.normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # applying multi-head attention.\n",
        "    attention = self.mha(x, x, x, padding_mask)\n",
        "    dropout1 = self.dropout1(attention, training=training)\n",
        "    normalization1 = self.normalization1(x + dropout1)\n",
        "\n",
        "    # applying the feed forward network.\n",
        "    ffn = self.ffn(normalization1)\n",
        "    dropout2 = self.dropout2(ffn, training=training)\n",
        "    output = self.normalization2(normalization1 + dropout2)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQwXR1u8sX3i",
        "colab_type": "text"
      },
      "source": [
        "Creating the Encoder layer of the Matching Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5rvsH0JcgiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, vocab_size, position, num_heads, dff, d_model, rate):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # creating the embedding and positional encoding layers.\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "    self.pe = PositionalEncoding(position, d_model)\n",
        "\n",
        "    # creating the dropout layer.    \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the encoder layers.    \n",
        "    self.encoder_layers = [EncoderLayer(num_heads, dff, d_model, rate) for index in range(num_layers)]\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # calculating the embeddings and applying the positional encoding.\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
        "    x = self.pe(x)\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for index in range(self.num_layers):\n",
        "      # stacking the encoder layers.\n",
        "      x = self.encoder_layers[index](x, padding_mask, training)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNs400xzs0Md",
        "colab_type": "text"
      },
      "source": [
        "Creating the Triangle's Area - Sector's Area Similarity layer of the Matching Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbXDVps3XR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TSSS(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(TSSS, self).__init__()\n",
        "\n",
        "  def call(self, x, y):\n",
        "    # calculating the theta angle.\n",
        "    x_normalized = tf.math.l2_normalize(x, axis=-1)\n",
        "    y_normalized = tf.math.l2_normalize(y, axis=-1)\n",
        "    theta = tf.acos(tf.matmul(x_normalized, y_normalized, transpose_b=True)) + tf.constant(m.radians(10))\n",
        "\n",
        "    # calculating the euclidean distance.\n",
        "    x_matrix = tf.reshape(tf.repeat(x, repeats=tf.shape(x)[-2], axis=-2), shape=[-1, tf.shape(x)[-2], tf.shape(x)[-1]])\n",
        "    y_matrix = tf.reshape(tf.tile(y, multiples=[tf.shape(y)[-2], 1]), shape=[-1, tf.shape(y)[-2], tf.shape(y)[-1]])\n",
        "    ed = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x_matrix, y_matrix)), axis=-1))\n",
        "\n",
        "    # calculating the magnitude difference.\n",
        "    x_sqrt = tf.sqrt(tf.reduce_sum(tf.square(x_matrix), axis=-1))\n",
        "    y_sqrt = tf.sqrt(tf.reduce_sum(tf.square(y_matrix), axis=-1))\n",
        "    md = tf.abs(x_sqrt - y_sqrt)\n",
        "\n",
        "    # calculating the Triangle's Area Similarity.\n",
        "    x_norm = tf.norm(x, ord='euclidean', axis=-1)[:, tf.newaxis]\n",
        "    y_norm = tf.norm(y, ord='euclidean', axis=-1)[:, tf.newaxis]\n",
        "    ts = (tf.matmul(x_norm, y_norm, transpose_b=True) * tf.sin(theta * (tf.constant(m.pi) / 180))) / 2\n",
        "\n",
        "    # calculating the Sector's Area Similarity.\n",
        "    ss = (tf.constant(m.pi) * tf.pow((ed + md), 2) * theta) / 360\n",
        "\n",
        "    # calculating the TS-SS.\n",
        "    output = ts * ss\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQq0sgHVs5k8",
        "colab_type": "text"
      },
      "source": [
        "Creating the model of the Matching Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3AZ1H7I_SA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatchingNetwork(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_layers, input_vocab_size, target_vocab_size, input_position, target_position, num_heads, dff, d_model, rate):\n",
        "    super(MatchingNetwork, self).__init__()\n",
        "\n",
        "    # creating the Encoders.\n",
        "    self.encoder1 = Encoder(num_layers, input_vocab_size, input_position, num_heads, dff, d_model, rate)\n",
        "    self.encoder2 = Encoder(num_layers, target_vocab_size, target_position, num_heads, dff, d_model, rate)\n",
        "\n",
        "    # creating the Triangle Sector Similarity layer.\n",
        "    self.similarity = TSSS()\n",
        "\n",
        "  def call(self, x, y, padding_mask_x, padding_mask_y, training):\n",
        "    # creating the encoded input padding mask.\n",
        "    mask1 = tf.squeeze(padding_mask_x)[:, :, tf.newaxis]\n",
        "    mask1 = -1e9 * (1 - mask1)\n",
        "\n",
        "    # passing the input data to its corresponding encoder.\n",
        "    encoded1 = self.encoder1(x, padding_mask_x, training)\n",
        "    encoded1 = encoded1 + mask1\n",
        "    encoded1 = tf.reduce_max(encoded1, axis=-2)\n",
        "\n",
        "    # creating the encoded target padding mask.\n",
        "    mask2 = tf.squeeze(padding_mask_y)[:, :, tf.newaxis]\n",
        "    mask2 = -1e9 * (1 - mask2)\n",
        "\n",
        "    # passing the target data to its corresponding encoder.\n",
        "    encoded2 = self.encoder2(y, padding_mask_y, training)\n",
        "    encoded2 = encoded2 + mask2\n",
        "    encoded2 = tf.reduce_max(encoded2, axis=-2)\n",
        "\n",
        "    # calculating the similarity and its invert.\n",
        "    similarity = self.similarity(encoded1, encoded2)\n",
        "    similarity = 1 / similarity\n",
        "\n",
        "    return similarity, encoded1, encoded2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlZUupx8tMVf",
        "colab_type": "text"
      },
      "source": [
        "Creating the Matching Network with specific hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGb49npMa_Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_LAYERS = 3\n",
        "INPUT_VOCAB_SIZE = input_encoder.vocab_size\n",
        "TARGET_VOCAB_SIZE = target_encoder.vocab_size\n",
        "INPUT_POSITION = input_encoder.vocab_size\n",
        "TARGET_POSITION = target_encoder.vocab_size\n",
        "NUM_HEADS = 8\n",
        "DFF = 512\n",
        "D_MODEL = 128\n",
        "RATE = 0.1\n",
        "\n",
        "matching_network = MatchingNetwork(NUM_LAYERS, INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE,\n",
        "                                   INPUT_POSITION, TARGET_POSITION, NUM_HEADS,\n",
        "                                   DFF, D_MODEL, RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl59n3IsSyCg",
        "colab_type": "text"
      },
      "source": [
        "Loading model's weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTGJ4h9LSyaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile('/content/drive/My Drive/TS-SS/weights.index'):\n",
        "  matching_network.load_weights('/content/drive/My Drive/TS-SS/weights')\n",
        "  print('Model restored.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o4kBtFatQ0Z",
        "colab_type": "text"
      },
      "source": [
        "Finding the best learning rate and creating the Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWFtHhZ7lZnf",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# finding the best learning rate using Exponential Decay.\n",
        "#learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-10, decay_steps=100, decay_rate=1.1, staircase=True)\n",
        "\n",
        "# creating the Adam optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(3.2e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXiokYr0tVJ4",
        "colab_type": "text"
      },
      "source": [
        "Defining the metrics of the training process.\n",
        "*   *SquaredMarginLoss*: calculates a squared margin-like loss for the output probabilities of the Matching Network. Margin is set to 5.\n",
        "*   *CategoricalAccuracy*: calculates the percentage of the correct predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et-3IeH0MmrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SquaredMarginLoss(predictions, margin=5):\n",
        "  # calculating the positive loss.\n",
        "  positive_loss = tf.linalg.diag_part(predictions)\n",
        "  positive_loss = tf.pow(tf.maximum(0., margin - positive_loss), 2)\n",
        "  \n",
        "  # calculating the negative loss. \n",
        "  diag_minus_infinity = tf.linalg.diag(tf.fill(dims=[tf.shape(predictions)[0]], value=-1e9))\n",
        "  negative_loss = tf.nn.relu(predictions + diag_minus_infinity)\n",
        "  negative_loss = tf.pow(negative_loss, 2)\n",
        "  negative_loss = tf.reduce_sum(negative_loss, axis=-1)\n",
        "\n",
        "  # summing both losses.\n",
        "  total_loss = positive_loss + negative_loss\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "# training metrics.\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "# validation metrics.\n",
        "valid_loss = tf.keras.metrics.Mean()\n",
        "valid_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4uU6kUstfcv",
        "colab_type": "text"
      },
      "source": [
        "Defining the Mean Reciprocal Rank (MRR) function. The MRR measures the rank of the correct prediction using the current position of the correct predictions. For example, for the first place it is 1, for the second place it is 1/2, for the third place it is 1/3 etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut6m0qXif8TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MRR(predictions):\n",
        "  # getting the correct predictions.\n",
        "  positive_scores = tf.linalg.diag_part(predictions)\n",
        "  # calculating their position in respect to the other predictions.\n",
        "  compared_scores = predictions >= tf.expand_dims(positive_scores, axis=-1)\n",
        "  # calculating the MRR metric.\n",
        "  mrr = 1 / tf.reduce_sum(tf.cast(compared_scores, dtype=tf.float32), axis=-1)\n",
        "\n",
        "  return mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I64r6ZGKtmKi",
        "colab_type": "text"
      },
      "source": [
        "Defining the training and validation step function. Both function use the @tf.function decorator to execute in graph mode and speed-up the training process using less resources.\n",
        "*   *train_step*: receives the predictions of the Matching Network, calculates the loss and the loss' gradients. Then it applies the gradients to the model's trainable variables. To avoid the exploding gradient phenomenon, gradient clipping is applied to the gradients. The function also calculates the accuracy and the MRR metrics for the predictions.\n",
        "*   *valid_step*: receives the predictions of the Matching Network and calculates the loss. The function also calculates the accuracy and the MRR metrics for the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feC9M1x6BUAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "  # creating the input padding mask.\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "  # creating the target padding mask.\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  # creating the ground truth for the accuracy metric.\n",
        "  diagonal = tf.ones(tf.shape(inp)[-2])\n",
        "  one_hot_y = tf.linalg.tensor_diag(diagonal)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, training=True)\n",
        "\n",
        "    loss = SquaredMarginLoss(predictions)\n",
        "  \n",
        "  # calculating and applying the gradients.\n",
        "  gradients = tape.gradient(loss, matching_network.trainable_variables)\n",
        "  gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
        "  optimizer.apply_gradients(zip(gradients, matching_network.trainable_variables))\n",
        "  \n",
        "  # calculating the metrics.\n",
        "  train_loss(loss)\n",
        "  train_accuracy(one_hot_y, predictions)\n",
        "  train_mrr = MRR(predictions)\n",
        "  \n",
        "  return train_mrr\n",
        "\n",
        "@tf.function\n",
        "def valid_step(inp, tar):\n",
        "  # creating the input padding mask.\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "  # creating the target padding mask.\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  # creating the ground truth for the accuracy metric.\n",
        "  diagonal = tf.ones(tf.shape(inp)[-2])\n",
        "  one_hot_y = tf.linalg.tensor_diag(diagonal)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, training=False)\n",
        "\n",
        "    loss = SquaredMarginLoss(predictions)\n",
        "  \n",
        "  # calculating the metrics.\n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(one_hot_y, predictions)\n",
        "  valid_mrr = MRR(predictions)\n",
        "\n",
        "  return valid_mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb1HH8dmt7WB",
        "colab_type": "text"
      },
      "source": [
        "Creating a summary writer for TensorBoard logging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJoN4X5CmHrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_writer = tf.summary.create_file_writer('logs/gradient_tape/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2n8aQ-qt9hM",
        "colab_type": "text"
      },
      "source": [
        "Creating the training loop. The loop:\n",
        "*   calls the training and validation step functions.\n",
        "*   outputs the metrics from both step functions for each batch and an average for each epoch.\n",
        "*   logs all metrics to TensorBoard.\n",
        "*   saves the weights of the best model taking into account the best validation MRR.\n",
        "*   calculates the time taken for each epoch, as well as the total training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MF5x8M8N1Wd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "stopwatch = []\n",
        "step = 0\n",
        "best_valid_mrr = 0\n",
        "\n",
        "for epoch in range(50):\n",
        "  # initializing the timer.\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the training metric storing lists.\n",
        "  epoch_train_loss = []\n",
        "  epoch_train_accuracy = []\n",
        "  epoch_train_mrr = []\n",
        "\n",
        "  # initializing the validation metric storing lists.\n",
        "  epoch_valid_loss = []\n",
        "  epoch_valid_accuracy = []\n",
        "  epoch_valid_mrr = []\n",
        "  \n",
        "  for batch, (inp, tar) in enumerate(train_dataset):\n",
        "    # resetting the loss and accuracy states for every training batch.\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # calling the training step function and storing the metrics.\n",
        "    epoch_train_mrr.append(tf.reduce_mean(train_step(inp, tar)))\n",
        "    epoch_train_loss.append(train_loss.result())\n",
        "    epoch_train_accuracy.append(train_accuracy.result())\n",
        "\n",
        "    # outputting the training metrics every 1000 batches.\n",
        "    if batch % 500 == 0:\n",
        "      print('Epoch {} Batch {} Train Loss {:.10f} Train Accuracy {:.10f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    # logging the training metrics every 100 batches.\n",
        "    if batch % 100 == 0:\n",
        "      with summary_writer.as_default():\n",
        "        tf.summary.scalar('train_loss', data=train_loss.result(), step=step)\n",
        "        tf.summary.scalar('train_accuracy', data=train_accuracy.result(), step=step)\n",
        "        #tf.summary.scalar('learning_rate', data=learning_rate(step), step=step)\n",
        "\n",
        "    step += 1\n",
        "\n",
        "  for inp, tar in valid_dataset:\n",
        "    # resetting the loss and accuracy states for every validation batch.\n",
        "    valid_loss.reset_states()\n",
        "    valid_accuracy.reset_states()\n",
        "\n",
        "    # calling the validation step function and storing the metrics.\n",
        "    epoch_valid_mrr.append(tf.reduce_mean(valid_step(inp, tar)))\n",
        "    epoch_valid_loss.append(valid_loss.result())\n",
        "    epoch_valid_accuracy.append(valid_accuracy.result())\n",
        "\n",
        "  # logging the average training and validation metrics every epoch.\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('epoch_train_loss', data=tf.reduce_mean(epoch_train_loss), step=epoch)\n",
        "    tf.summary.scalar('epoch_train_accuracy', data=tf.reduce_mean(epoch_train_accuracy), step=epoch)\n",
        "    tf.summary.scalar('epoch_train_mrr', data=tf.reduce_mean(epoch_train_mrr), step=epoch)\n",
        "    tf.summary.scalar('epoch_valid_loss', data=tf.reduce_mean(epoch_valid_loss), step=epoch)\n",
        "    tf.summary.scalar('epoch_valid_accuracy', data=tf.reduce_mean(epoch_valid_accuracy), step=epoch)\n",
        "    tf.summary.scalar('epoch_valid_mrr', data=tf.reduce_mean(epoch_valid_mrr), step=epoch)\n",
        "\n",
        "  # outputting the average training metrics every epoch.\n",
        "  print('Epoch {} Train Loss {:.10f} Train Accuracy {:.10f} Train MRR {:.10f}'.format(epoch + 1, \n",
        "                                                tf.reduce_mean(epoch_train_loss), \n",
        "                                                tf.reduce_mean(epoch_train_accuracy),\n",
        "                                                tf.reduce_mean(epoch_train_mrr)))\n",
        "  \n",
        "  # outputting the average validation metrics every epoch.\n",
        "  print('Epoch {} Valid Loss {:.10f} Valid Accuracy {:.10f} Valid MRR {:.10f}'. format(epoch + 1,\n",
        "                                                tf.reduce_mean(epoch_valid_loss), \n",
        "                                                tf.reduce_mean(epoch_valid_accuracy),\n",
        "                                                tf.reduce_mean(epoch_valid_mrr)))\n",
        "\n",
        "  # outputting the epoch time.\n",
        "  print('Time taken for 1 epoch: {} seconds\\n'.format(time.time() - start))\n",
        "  stopwatch.append(time.time() - start)\n",
        "\n",
        "  # saving the best model weights.\n",
        "  if tf.reduce_mean(epoch_valid_mrr) > best_valid_mrr:\n",
        "    best_valid_mrr = tf.reduce_mean(epoch_valid_mrr)\n",
        "\n",
        "    matching_network.save_weights('/content/drive/My Drive/TS-SS/weights', overwrite=True)\n",
        "    print('Model saved at epoch {}\\n'.format(epoch+1))\n",
        "\n",
        "# outputting the total training time.\n",
        "print('Total training time: {} seconds\\n'.format(tf.reduce_sum(stopwatch)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5QdMx1CueaO",
        "colab_type": "text"
      },
      "source": [
        "Syncing the W&B run with Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3YejNPUBwHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb sync logs/gradient_tape/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJDDOCBNuf6O",
        "colab_type": "text"
      },
      "source": [
        "Opening the TensorBoard panel for log reviewing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpXW8lYubuMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/gradient_tape/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUeVf8H0uh_4",
        "colab_type": "text"
      },
      "source": [
        "Defining the raw input preprocessing functions.\n",
        "*   *remove_special*: replaces all special characters in the raw input with an empty string.\n",
        "*   *remove_empty*: removes all empty strings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nTJAvtPVcUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_special(data):\n",
        "  for index, row in enumerate(data):\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_empty(data):\n",
        "  for index, row in enumerate(data):\n",
        "    for token in row:\n",
        "      if not token:  \n",
        "        # removing empty strings from the list.\n",
        "        data[index] = list(filter(None, row))\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA2rb29vukHt",
        "colab_type": "text"
      },
      "source": [
        "Reading the 99 queries from a TXT file and preprocessing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5LzWSGyTTV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/queries.txt', 'r') as f:\n",
        "    queries_file = f.readlines()\n",
        "\n",
        "# removing newline characters.\n",
        "queries_file = [[line.strip()] for line in queries_file]\n",
        "# tokenizing the queries.\n",
        "queries = [token.split() for line in queries_file for token in line]\n",
        "queries = remove_special(queries)\n",
        "queries = remove_empty(queries)\n",
        "\n",
        "# applying encoding to the queries.\n",
        "encoded_queries, _ = encode(queries, [], input_encoder, None)\n",
        "# converting the queries to numpy.\n",
        "encoded_queries, _ = to_numpy(encoded_queries, [])\n",
        "# creating a constant tensor with the encoded queries.\n",
        "queries_set = tf.constant(encoded_queries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtGlTlNFlrz_",
        "colab_type": "text"
      },
      "source": [
        "Creating and storing the vector representations of the 99 queries and the whole corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EmFg22za7_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the input padding mask.\n",
        "padding_mask_inp = 1 - tf.cast(tf.equal(queries_set, 0), dtype=tf.float32)\n",
        "padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "_, query_representations, _ = matching_network(queries_set, queries_set, padding_mask_inp, padding_mask_inp, False)\n",
        "\n",
        "function_representations = []\n",
        "\n",
        "for tar in corpus_dataset:\n",
        "  inp = tf.ones_like(tar)\n",
        "\n",
        "  # creating the input padding mask.\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "  # creating the target padding mask.\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  _, _, function_vectors = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, False)\n",
        "\n",
        "  # storing the function vectors in a list.\n",
        "  function_representations.append(function_vectors)\n",
        "\n",
        "# concatenating all vectors on the first axis.\n",
        "function_representations = tf.concat(function_representations, axis=-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPgPx3WEu5ZF",
        "colab_type": "text"
      },
      "source": [
        "Adding all function vectors to the AnnoyIndex, building the trees and saving the ANN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG6atpJGUidQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = AnnoyIndex(tf.shape(function_representations)[-1], 'euclidean')\n",
        "\n",
        "for index, vector in enumerate(function_representations):\n",
        "  indices.add_item(index, vector)\n",
        "\n",
        "indices.build(10)\n",
        "indices.save('/content/drive/My Drive/TS-SS/functions.ann')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSFPmP0ou649",
        "colab_type": "text"
      },
      "source": [
        "Getting the top 100 predictions for the 99 queries using Annoy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUHa1LQmXBJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(vector, indices):\n",
        "  function_index, distance = indices.get_nns_by_vector(vector, n=100, include_distances=True)\n",
        "\n",
        "  return function_index, distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBDPQK0bu9Ee",
        "colab_type": "text"
      },
      "source": [
        "Storing the queries, their predictions, as well as the prediction URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhEgdcioVYve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for query_index, vector in enumerate(query_representations):\n",
        "  function_index, distance = get_predictions(vector, indices)\n",
        "  \n",
        "  for index in function_index:\n",
        "    predictions.append([queries_file[query_index][0], 'java', functions.url[index]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hButYw5yu-sA",
        "colab_type": "text"
      },
      "source": [
        "Creating a DataFrame of the predictions and exporting it as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COQ-7fpLYxd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_dataframe = pd.DataFrame(predictions, columns=['query', 'language', 'url'])\n",
        "predictions_dataframe.to_csv('/content/drive/My Drive/TS-SS/model_predictions.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-dzILPIvAGa",
        "colab_type": "text"
      },
      "source": [
        "Saving the CSV file to the W&B run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeZmGYcEte61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.save('/content/drive/My Drive/TS-SS/model_predictions.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U_eFtBGvBc6",
        "colab_type": "text"
      },
      "source": [
        "Testing section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJoR5ZLMI1dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 0\n",
        "\n",
        "for inp, tar in train_dataset:\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  out, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, False)\n",
        "\n",
        "  i += 1\n",
        "  if i == 1:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuYTy2cPwLY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uku-igHSoi7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = en1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfovqgTVHf0h",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "t2 = en2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZagwESsnaEBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsss = TSSS()\n",
        "result = tsss(t1, t2)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}