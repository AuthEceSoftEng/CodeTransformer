{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cosine_matching_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPl9DIaPh1VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFINh3zdgnJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52uo3-n9h7-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "wandb.init(project='cosine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSfzHqa6Sm4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "import math as m\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaaMa8H-cN7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfF4zQM_SrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the dataset.\n",
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset.pkl')\n",
        "\n",
        "# copying docstring_tokens column.\n",
        "docstring_tokens = dataset['docstring_tokens'].copy(deep=True)\n",
        "# copying function_tokens column.\n",
        "function_tokens = dataset['function_tokens'].copy(deep=True)\n",
        "\n",
        "# loading the docstring vocabulary.\n",
        "docstring_vocab =  pickle.load(open('/content/drive/My Drive/docstring_vocab.pkl', 'rb'))\n",
        "# loading the function vocabulary.\n",
        "function_vocab =  pickle.load(open('/content/drive/My Drive/function_vocab.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELRc6XXkNyQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(inp, tar, input_encoder, target_encoder):\n",
        "  # encoding input data.\n",
        "  for index, row in enumerate(inp):\n",
        "    inp[index] = [input_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  # encoding target data.\n",
        "  for index, row in enumerate(tar):\n",
        "    tar[index] = [target_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  return inp, tar\n",
        "\n",
        "def to_numpy(inp, tar):\n",
        "  # converting input data to numpy.\n",
        "  inp = pd.DataFrame(list(inp))\n",
        "  inp = inp.to_numpy()\n",
        "  inp = np.nan_to_num(inp)\n",
        "  inp = inp.astype(int)\n",
        "\n",
        "  # converting target data to numpy.\n",
        "  tar = pd.DataFrame(list(tar))\n",
        "  tar = tar.to_numpy()\n",
        "  tar = np.nan_to_num(tar)\n",
        "  tar = tar.astype(int)\n",
        "\n",
        "  return inp, tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLmqTMk4NzTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building input_encoder.\n",
        "input_encoder = tfds.features.text.TokenTextEncoder(docstring_vocab)\n",
        "# building target_encoder.\n",
        "target_encoder = tfds.features.text.TokenTextEncoder(function_vocab)\n",
        "\n",
        "# initializing encoded_input list of lists.\n",
        "encoded_input = docstring_tokens.copy(deep=True)\n",
        "# initializing encoded_target list.\n",
        "encoded_target = function_tokens.copy(deep=True)\n",
        "\n",
        "# applying encoding to input and target data.\n",
        "encoded_input, encoded_target = encode(encoded_input, encoded_target, input_encoder, target_encoder)\n",
        "# converting input and target data to numpy.\n",
        "encoded_input, encoded_target = to_numpy(encoded_input, encoded_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3aKCL3mcM-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# creating a tensor dataset with the input data.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((encoded_input, encoded_target))\n",
        "# caching the dataset for performance optimizations.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5jo2SY2cbAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # creating the positional encoding matrix.\n",
        "    self.pe = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # storing word positions to a matrix.\n",
        "    position = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
        "    # storing embedding components to a matrix.\n",
        "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
        "\n",
        "    # calculating the angles.\n",
        "    angle = tf.multiply(position, 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, dtype=tf.float32)))\n",
        "\n",
        "    # applying sine to the angles of even indices.\n",
        "    sines = tf.sin(angle[:, 0::2])\n",
        "    # applying cosine to the angles of odd indices.\n",
        "    cosines = tf.cos(angle[:, 1::2])\n",
        "\n",
        "    # concatenating sines and cosines in one matrix.\n",
        "    pe = tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pe, dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    # adding positional encoding to the input embeddings on call.\n",
        "    return x + self.pe[:, :tf.shape(x)[-2], :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tTskI9ccaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, d_model):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    self.head_size = d_model // num_heads\n",
        "\n",
        "    # creating the weight matrices for each head.\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # creating the weight matrix for the output.\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, query, key, value, mask):\n",
        "    # storing the batch size.\n",
        "    batch_size = tf.shape(query)[-3]\n",
        "\n",
        "    # passing query, key and value as input to the weight matrices.\n",
        "    query = self.wq(query)\n",
        "    key = self.wk(key)\n",
        "    value = self.wv(value)\n",
        "\n",
        "    # splitting the dense tensors for each head.\n",
        "    query = tf.reshape(query, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    key = tf.reshape(key, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    value = tf.reshape(value, [batch_size, -1, self.num_heads, self.head_size])\n",
        "\n",
        "    # transposing the number of heads and sequence length columns.\n",
        "    query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "    key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "    value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # calculating the similarity score.\n",
        "    query_keyT = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # calculating the depth.\n",
        "    depth = tf.cast(tf.shape(key)[-1], dtype=tf.float32)\n",
        "    # calculating the scale factor.\n",
        "    scale = 1 / tf.sqrt(depth)\n",
        "\n",
        "    # calculating the scaled similarity scores.\n",
        "    scores = query_keyT * scale\n",
        "\n",
        "    # masking out key/value pairs.\n",
        "    if mask is not None:\n",
        "      scores *= mask\n",
        "      scores = tf.where(tf.equal(scores, 0), tf.ones_like(scores) * -1e9, scores)\n",
        "\n",
        "    # calculating the scaled similarity scores' softmax matrix.\n",
        "    softmax = tf.nn.softmax(scores)\n",
        "\n",
        "    # calculating the scaled dot-product attention for each head.\n",
        "    attention = tf.matmul(softmax, value)\n",
        "    attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "\n",
        "    # concatenating the attention heads.\n",
        "    output = tf.reshape(attention, [batch_size, -1, self.d_model])\n",
        "    # passing the concatenation as input to a dense layer.\n",
        "    output = self.dense(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyoVUxxrcd41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dff, d_model):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "    # creating the dense layers of the feed forward network.\n",
        "    self.fc1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    # applying the layer with dff units and relu activation.\n",
        "    fc1 = self.fc1(x)\n",
        "    # applying the layer with d_model units and no activation.\n",
        "    output = self.fc2(fc1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIACE_slcfAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, dff, d_model, rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    # creating the MHA and FFN layers.\n",
        "    self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "    self.ffn = FeedForwardNetwork(dff, d_model)\n",
        "\n",
        "    # creating the dropout layers.\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the normalization layers.\n",
        "    self.normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # applying multi-head attention, dropout and then normalization.\n",
        "    attention = self.mha(x, x, x, padding_mask)\n",
        "    dropout1 = self.dropout1(attention, training=training)\n",
        "    normalization1 = self.normalization1(x + dropout1)\n",
        "\n",
        "    # applying the feed forward network.\n",
        "    ffn = self.ffn(normalization1)\n",
        "    dropout2 = self.dropout2(ffn, training=training)\n",
        "    output = self.normalization2(normalization1 + dropout2)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5rvsH0JcgiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, vocab_size, position, num_heads, dff, d_model, rate):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # creating the embedding and positional encoding layers.\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "    self.pe = PositionalEncoding(position, d_model)\n",
        "\n",
        "    # creating the dropout layer.    \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the encoder layers.    \n",
        "    self.encoder_layers = [EncoderLayer(num_heads, dff, d_model, rate) for index in range(num_layers)]\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # calculating the embeddings and applying the positional encoding.\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
        "    x = self.pe(x)\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for index in range(self.num_layers):\n",
        "      # stacking the encoder layers.\n",
        "      x = self.encoder_layers[index](x, padding_mask, training)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0dXCDQTvIYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cosine(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Cosine, self).__init__()\n",
        "\n",
        "  def call(self, x, y):\n",
        "    # calculating the cosine.\n",
        "    x_normalized = tf.math.l2_normalize(x, axis=-1)\n",
        "    y_normalized = tf.math.l2_normalize(y, axis=-1)\n",
        "    theta = tf.matmul(x_normalized, y_normalized, transpose_b=True)\n",
        "    \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3AZ1H7I_SA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatchingNetwork(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_layers, input_vocab_size, target_vocab_size, input_position, target_position, num_heads, dff, d_model, rate):\n",
        "    super(MatchingNetwork, self).__init__()\n",
        "\n",
        "    self.encoder1 = Encoder(num_layers, input_vocab_size, input_position, num_heads, dff, d_model, rate)\n",
        "    self.encoder2 = Encoder(num_layers, target_vocab_size, target_position, num_heads, dff, d_model, rate)\n",
        "\n",
        "    self.similarity = Cosine()\n",
        "\n",
        "  def call(self, x, y, padding_mask_x, padding_mask_y, training, inference):\n",
        "    mask1 = tf.squeeze(padding_mask_x)[:, :, tf.newaxis]\n",
        "    mask1 = -1e9 * (1 - mask1)\n",
        "\n",
        "    encoded1 = self.encoder1(x, padding_mask_x, training)\n",
        "    encoded1 = encoded1 + mask1\n",
        "    encoded1 = tf.reduce_max(encoded1, axis=-2)\n",
        "    encoded1 = tf.keras.activations.tanh(encoded1)\n",
        "\n",
        "    if not inference:\n",
        "      mask2 = tf.squeeze(padding_mask_y)[:, :, tf.newaxis]\n",
        "      mask2 = -1e9 * (1 - mask2)\n",
        "\n",
        "      encoded2 = self.encoder2(y, padding_mask_y, training)\n",
        "      encoded2 = encoded2 + mask2\n",
        "      encoded2 = tf.reduce_max(encoded2, axis=-2)\n",
        "      encoded2 = tf.keras.activations.tanh(encoded2)\n",
        "\n",
        "      similarity = self.similarity(encoded1, encoded2)\n",
        "      similarity = tf.nn.softmax(similarity, axis=-1)\n",
        "\n",
        "      return similarity, encoded1, encoded2\n",
        "\n",
        "    elif inference:\n",
        "      similarity = self.similarity(encoded1, y)\n",
        "      similarity = tf.nn.softmax(similarity, axis=-1)\n",
        "\n",
        "      return similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGb49npMa_Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_LAYERS = 4\n",
        "INPUT_VOCAB_SIZE = input_encoder.vocab_size\n",
        "TARGET_VOCAB_SIZE = target_encoder.vocab_size\n",
        "INPUT_POSITION = input_encoder.vocab_size\n",
        "TARGET_POSITION = target_encoder.vocab_size\n",
        "NUM_HEADS = 8\n",
        "DFF = 512\n",
        "D_MODEL = 128\n",
        "RATE = 0.1\n",
        "\n",
        "matching_network = MatchingNetwork(NUM_LAYERS, INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE,\n",
        "                                   INPUT_POSITION, TARGET_POSITION, NUM_HEADS,\n",
        "                                   DFF, D_MODEL, RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJoN4X5CmHrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWFtHhZ7lZnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-10, decay_steps=100, decay_rate=1.3, staircase=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(7.5e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et-3IeH0MmrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BoostedProbabilityLoss(predictions):\n",
        "  positive_loss = tf.linalg.diag_part(predictions)\n",
        "  positive_loss = 1 - tf.pow(positive_loss, 2)\n",
        "  \n",
        "  diag_minus_infinity = tf.linalg.diag(tf.fill(dims=[tf.shape(predictions)[0]], value=-1e9))\n",
        "  negative_loss = tf.nn.relu(predictions + diag_minus_infinity)\n",
        "  negative_loss = tf.pow(negative_loss, 2)\n",
        "  negative_loss = tf.reduce_sum(negative_loss, axis=-1)\n",
        "\n",
        "  total_loss = positive_loss + negative_loss\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feC9M1x6BUAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  diagonal = tf.ones(tf.shape(inp)[-2])\n",
        "  one_hot_y = tf.linalg.tensor_diag(diagonal)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, training=True, inference=False)\n",
        "\n",
        "    loss = BoostedProbabilityLoss(predictions)\n",
        "  \n",
        "  gradients = tape.gradient(loss, matching_network.trainable_variables)\n",
        "  gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
        "  optimizer.apply_gradients(zip(gradients, matching_network.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(one_hot_y, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MF5x8M8N1Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "step = 0\n",
        "\n",
        "for epoch in range(500):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  epoch_train_loss = []\n",
        "  epoch_train_accuracy = []\n",
        "  \n",
        "  for batch, (inp, tar) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    epoch_train_loss.append(train_loss.result())\n",
        "    epoch_train_accuracy.append(train_accuracy.result())\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.10f} Accuracy {:.10f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "    if batch % 10 == 0:\n",
        "      with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', data=train_loss.result(), step=step)\n",
        "        tf.summary.scalar('accuracy', data=train_accuracy.result(), step=step)\n",
        "        #tf.summary.scalar('learning_rate', data=learning_rate(step), step=step)\n",
        "\n",
        "      wandb.log({'loss': train_loss.result(), 'accuracy': train_accuracy.result()}, step=step)\n",
        "\n",
        "    step += 1\n",
        "      \n",
        "  print ('Epoch {} Loss {:.10f} Accuracy {:.10f}'.format(epoch + 1, \n",
        "                                                tf.reduce_mean(epoch_train_loss), \n",
        "                                                tf.reduce_mean(epoch_train_accuracy)))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpXW8lYubuMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/gradient_tape/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJoR5ZLMI1dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing\n",
        "i = 0\n",
        "for inp, tar in train_dataset:\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  out, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, False, False)\n",
        "\n",
        "  i += 1\n",
        "  if i == 1:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZQVPussly_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uku-igHSoi7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = en1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfovqgTVHf0h",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "t2 = en2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqSmdhYmIUUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine = Cosine()\n",
        "result = cosine(t1, t2)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}