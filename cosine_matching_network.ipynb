{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cosine_matching_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy2P3_blneLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install annoy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPl9DIaPh1VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSfzHqa6Sm4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "import re\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "import math as m\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from annoy import AnnoyIndex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52uo3-n9h7-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n",
        "wandb.init(project='self-attention-cosine')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaaMa8H-cN7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfF4zQM_SrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading the dataset.\n",
        "dataset = pd.read_pickle('/content/drive/My Drive/dataset.pkl')\n",
        "\n",
        "# copying docstring_tokens column.\n",
        "docstring_tokens = dataset['docstring_tokens'].copy(deep=True)\n",
        "# copying function_tokens column.\n",
        "function_tokens = dataset['function_tokens'].copy(deep=True)\n",
        "\n",
        "# loading the docstring vocabulary.\n",
        "docstring_vocab =  pickle.load(open('/content/drive/My Drive/docstring_vocab.pkl', 'rb'))\n",
        "# loading the function vocabulary.\n",
        "function_vocab =  pickle.load(open('/content/drive/My Drive/function_vocab.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELRc6XXkNyQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(inp, tar, input_encoder, target_encoder):\n",
        "  # encoding input data.\n",
        "  for index, row in enumerate(inp):\n",
        "    inp[index] = [input_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  # encoding target data.\n",
        "  for index, row in enumerate(tar):\n",
        "    tar[index] = [target_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  return inp, tar\n",
        "\n",
        "def to_numpy(inp, tar):\n",
        "  # converting input data to numpy.\n",
        "  inp = pd.DataFrame(list(inp))\n",
        "  inp = inp.to_numpy()\n",
        "  inp = np.nan_to_num(inp)\n",
        "  inp = inp.astype(int)\n",
        "\n",
        "  # converting target data to numpy.\n",
        "  tar = pd.DataFrame(list(tar))\n",
        "  tar = tar.to_numpy()\n",
        "  tar = np.nan_to_num(tar)\n",
        "  tar = tar.astype(int)\n",
        "\n",
        "  return inp, tar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLmqTMk4NzTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# building input_encoder.\n",
        "input_encoder = tfds.features.text.TokenTextEncoder(docstring_vocab)\n",
        "# building target_encoder.\n",
        "target_encoder = tfds.features.text.TokenTextEncoder(function_vocab)\n",
        "\n",
        "# initializing original_input list of lists.\n",
        "original_input = docstring_tokens.copy(deep=True)\n",
        "# initializing original_target list.\n",
        "original_target = function_tokens.copy(deep=True)\n",
        "\n",
        "# splitting to training set.\n",
        "training_input = original_input[:480000]\n",
        "training_target = original_target[:480000]\n",
        "# splitting to validation set.\n",
        "validation_input = original_input[480000:]\n",
        "validation_input = validation_input.reset_index(drop=True)\n",
        "validation_target = original_target[480000:]\n",
        "validation_target = validation_target.reset_index(drop=True)\n",
        "\n",
        "# applying encoding to input and target data.\n",
        "encoded_training_input, encoded_training_target = encode(training_input, training_target, input_encoder, target_encoder)\n",
        "encoded_validation_input, encoded_validation_target = encode(validation_input, validation_target, input_encoder, target_encoder)\n",
        "# converting input and target data to numpy.\n",
        "encoded_training_input, encoded_training_target = to_numpy(encoded_training_input, encoded_training_target)\n",
        "encoded_validation_input, encoded_validation_target = to_numpy(encoded_validation_input, encoded_validation_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3aKCL3mcM-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# creating a tensor dataset with the training data.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((encoded_training_input, encoded_training_target))\n",
        "# caching the dataset for performance optimizations.\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# creating a tensor dataset with the validation data.\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((encoded_validation_input, encoded_validation_target))\n",
        "# caching the dataset for performance optimizations.\n",
        "valid_dataset = valid_dataset.cache()\n",
        "valid_dataset = valid_dataset.batch(BATCH_SIZE)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5jo2SY2cbAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # creating the positional encoding matrix.\n",
        "    self.pe = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # storing word positions to a matrix.\n",
        "    position = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
        "    # storing embedding components to a matrix.\n",
        "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
        "\n",
        "    # calculating the angles.\n",
        "    angle = tf.multiply(position, 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, dtype=tf.float32)))\n",
        "\n",
        "    # applying sine to the angles of even indices.\n",
        "    sines = tf.sin(angle[:, 0::2])\n",
        "    # applying cosine to the angles of odd indices.\n",
        "    cosines = tf.cos(angle[:, 1::2])\n",
        "\n",
        "    # concatenating sines and cosines in one matrix.\n",
        "    pe = tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pe, dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    # adding positional encoding to the input embeddings on call.\n",
        "    return x + self.pe[:, :tf.shape(x)[-2], :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tTskI9ccaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, d_model):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    self.head_size = d_model // num_heads\n",
        "\n",
        "    # creating the weight matrices for each head.\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # creating the weight matrix for the output.\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, query, key, value, mask):\n",
        "    # storing the batch size.\n",
        "    batch_size = tf.shape(query)[-3]\n",
        "\n",
        "    # passing query, key and value as input to the weight matrices.\n",
        "    query = self.wq(query)\n",
        "    key = self.wk(key)\n",
        "    value = self.wv(value)\n",
        "\n",
        "    # splitting the dense tensors for each head.\n",
        "    query = tf.reshape(query, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    key = tf.reshape(key, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    value = tf.reshape(value, [batch_size, -1, self.num_heads, self.head_size])\n",
        "\n",
        "    # transposing the number of heads and sequence length columns.\n",
        "    query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "    key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "    value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # calculating the similarity score.\n",
        "    query_keyT = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # calculating the depth.\n",
        "    depth = tf.cast(tf.shape(key)[-1], dtype=tf.float32)\n",
        "    # calculating the scale factor.\n",
        "    scale = 1 / tf.sqrt(depth)\n",
        "\n",
        "    # calculating the scaled similarity scores.\n",
        "    scores = query_keyT * scale\n",
        "\n",
        "    # masking out key/value pairs.\n",
        "    if mask is not None:\n",
        "      scores *= mask\n",
        "      scores = tf.where(tf.equal(scores, 0), tf.ones_like(scores) * -1e9, scores)\n",
        "\n",
        "    # calculating the scaled similarity scores' softmax matrix.\n",
        "    softmax = tf.nn.softmax(scores)\n",
        "\n",
        "    # calculating the scaled dot-product attention for each head.\n",
        "    attention = tf.matmul(softmax, value)\n",
        "    attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "\n",
        "    # concatenating the attention heads.\n",
        "    output = tf.reshape(attention, [batch_size, -1, self.d_model])\n",
        "    # passing the concatenation as input to a dense layer.\n",
        "    output = self.dense(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyoVUxxrcd41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dff, d_model):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "    # creating the dense layers of the feed forward network.\n",
        "    self.fc1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    # applying the layer with dff units and relu activation.\n",
        "    fc1 = self.fc1(x)\n",
        "    # applying the layer with d_model units and no activation.\n",
        "    output = self.fc2(fc1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIACE_slcfAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, dff, d_model, rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    # creating the MHA and FFN layers.\n",
        "    self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "    self.ffn = FeedForwardNetwork(dff, d_model)\n",
        "\n",
        "    # creating the dropout layers.\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the normalization layers.\n",
        "    self.normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # applying multi-head attention, dropout and then normalization.\n",
        "    attention = self.mha(x, x, x, padding_mask)\n",
        "    dropout1 = self.dropout1(attention, training=training)\n",
        "    normalization1 = self.normalization1(x + dropout1)\n",
        "\n",
        "    # applying the feed forward network.\n",
        "    ffn = self.ffn(normalization1)\n",
        "    dropout2 = self.dropout2(ffn, training=training)\n",
        "    output = self.normalization2(normalization1 + dropout2)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5rvsH0JcgiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, vocab_size, position, num_heads, dff, d_model, rate):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # creating the embedding and positional encoding layers.\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "    self.pe = PositionalEncoding(position, d_model)\n",
        "\n",
        "    # creating the dropout layer.    \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the encoder layers.    \n",
        "    self.encoder_layers = [EncoderLayer(num_heads, dff, d_model, rate) for index in range(num_layers)]\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # calculating the embeddings and applying the positional encoding.\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
        "    x = self.pe(x)\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for index in range(self.num_layers):\n",
        "      # stacking the encoder layers.\n",
        "      x = self.encoder_layers[index](x, padding_mask, training)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0dXCDQTvIYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Cosine(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(Cosine, self).__init__()\n",
        "\n",
        "  def call(self, x, y):\n",
        "    # calculating the cosine.\n",
        "    x_normalized = tf.math.l2_normalize(x, axis=-1)\n",
        "    y_normalized = tf.math.l2_normalize(y, axis=-1)\n",
        "    theta = tf.matmul(x_normalized, y_normalized, transpose_b=True)\n",
        "    \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3AZ1H7I_SA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MatchingNetwork(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_layers, input_vocab_size, target_vocab_size, input_position, target_position, num_heads, dff, d_model, rate):\n",
        "    super(MatchingNetwork, self).__init__()\n",
        "\n",
        "    self.encoder1 = Encoder(num_layers, input_vocab_size, input_position, num_heads, dff, d_model, rate)\n",
        "    self.encoder2 = Encoder(num_layers, target_vocab_size, target_position, num_heads, dff, d_model, rate)\n",
        "\n",
        "    self.similarity = Cosine()\n",
        "\n",
        "  def call(self, x, y, padding_mask_x, padding_mask_y, training):\n",
        "    mask1 = tf.squeeze(padding_mask_x)[:, :, tf.newaxis]\n",
        "    mask1 = -1e9 * (1 - mask1)\n",
        "\n",
        "    encoded1 = self.encoder1(x, padding_mask_x, training)\n",
        "    encoded1 = encoded1 + mask1\n",
        "    encoded1 = tf.reduce_max(encoded1, axis=-2)\n",
        "    encoded1 = tf.keras.activations.tanh(encoded1)\n",
        "\n",
        "    mask2 = tf.squeeze(padding_mask_y)[:, :, tf.newaxis]\n",
        "    mask2 = -1e9 * (1 - mask2)\n",
        "\n",
        "    encoded2 = self.encoder2(y, padding_mask_y, training)\n",
        "    encoded2 = encoded2 + mask2\n",
        "    encoded2 = tf.reduce_max(encoded2, axis=-2)\n",
        "    encoded2 = tf.keras.activations.tanh(encoded2)\n",
        "\n",
        "    similarity = self.similarity(encoded1, encoded2)\n",
        "    similarity = tf.nn.softmax(similarity, axis=-1)\n",
        "\n",
        "    return similarity, encoded1, encoded2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGb49npMa_Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_LAYERS = 4\n",
        "INPUT_VOCAB_SIZE = input_encoder.vocab_size\n",
        "TARGET_VOCAB_SIZE = target_encoder.vocab_size\n",
        "INPUT_POSITION = input_encoder.vocab_size\n",
        "TARGET_POSITION = target_encoder.vocab_size\n",
        "NUM_HEADS = 8\n",
        "DFF = 512\n",
        "D_MODEL = 128\n",
        "RATE = 0.1\n",
        "\n",
        "matching_network = MatchingNetwork(NUM_LAYERS, INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE,\n",
        "                                   INPUT_POSITION, TARGET_POSITION, NUM_HEADS,\n",
        "                                   DFF, D_MODEL, RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWFtHhZ7lZnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-10, decay_steps=100, decay_rate=1.1, staircase=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(1.4e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5NWhgXK7D2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ckpt = tf.train.Checkpoint(matching_network=matching_network, optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, '/content/drive/My Drive/COSINE', max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Model restored.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et-3IeH0MmrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BoostedMarginLoss(predictions, margin=1):\n",
        "  positive_loss = tf.linalg.diag_part(predictions)\n",
        "  positive_loss = tf.pow(tf.maximum(0., margin - positive_loss), 2)\n",
        "  \n",
        "  diag_minus_infinity = tf.linalg.diag(tf.fill(dims=[tf.shape(predictions)[0]], value=-1e9))\n",
        "  negative_loss = tf.nn.relu(predictions + diag_minus_infinity)\n",
        "  negative_loss = tf.pow(negative_loss, 2)\n",
        "  negative_loss = tf.reduce_sum(negative_loss, axis=-1)\n",
        "\n",
        "  total_loss = positive_loss + negative_loss\n",
        "\n",
        "  return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "valid_loss = tf.keras.metrics.Mean()\n",
        "valid_accuracy = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC11P-YVKf13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MRR(predictions):\n",
        "  positive_scores = tf.linalg.diag_part(predictions)\n",
        "\n",
        "  compared_scores = predictions >= tf.expand_dims(positive_scores, axis=-1)\n",
        "\n",
        "  mrr = 1 / tf.reduce_sum(tf.cast(compared_scores, dtype=tf.float32), axis=-1)\n",
        "\n",
        "  return mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feC9M1x6BUAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  diagonal = tf.ones(tf.shape(inp)[-2])\n",
        "  one_hot_y = tf.linalg.tensor_diag(diagonal)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, training=True)\n",
        "\n",
        "    loss = BoostedMarginLoss(predictions)\n",
        "  \n",
        "  gradients = tape.gradient(loss, matching_network.trainable_variables)\n",
        "  gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
        "  optimizer.apply_gradients(zip(gradients, matching_network.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(one_hot_y, predictions)\n",
        "  train_mrr = MRR(predictions)\n",
        "  \n",
        "  return train_mrr\n",
        "\n",
        "@tf.function\n",
        "def valid_step(inp, tar):\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  diagonal = tf.ones(tf.shape(inp)[-2])\n",
        "  one_hot_y = tf.linalg.tensor_diag(diagonal)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, training=True)\n",
        "\n",
        "    loss = BoostedMarginLoss(predictions)\n",
        "  \n",
        "  valid_loss(loss)\n",
        "  valid_accuracy(one_hot_y, predictions)\n",
        "  valid_mrr = MRR(predictions)\n",
        "\n",
        "  return valid_mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJoN4X5CmHrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "summary_writer = tf.summary.create_file_writer('logs/gradient_tape/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MF5x8M8N1Wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwatch = []\n",
        "step = 0\n",
        "best_valid_mrr = 0\n",
        "\n",
        "for epoch in range(50):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  epoch_train_loss = []\n",
        "  epoch_train_accuracy = []\n",
        "\n",
        "  epoch_valid_mrr = []\n",
        "  epoch_valid_loss = []\n",
        "  epoch_valid_accuracy = []\n",
        "  \n",
        "  for batch, (inp, tar) in enumerate(train_dataset):\n",
        "    train_mrr = train_step(inp, tar)\n",
        "\n",
        "    epoch_train_loss.append(train_loss.result())\n",
        "    epoch_train_accuracy.append(train_accuracy.result())\n",
        "\n",
        "    if batch % 1000 == 0:\n",
        "      print('Epoch {} Batch {} Train Loss {:.10f} Train Accuracy {:.10f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "    if batch % 100 == 0:\n",
        "      with summary_writer.as_default():\n",
        "        tf.summary.scalar('train_loss', data=train_loss.result(), step=step)\n",
        "        tf.summary.scalar('train_mrr', data=tf.reduce_mean(train_mrr), step=step)\n",
        "        tf.summary.scalar('train_accuracy', data=train_accuracy.result(), step=step)\n",
        "        #tf.summary.scalar('learning_rate', data=learning_rate(step), step=step)\n",
        "\n",
        "    step += 1\n",
        "\n",
        "  for inp, tar in valid_dataset:\n",
        "    epoch_valid_mrr.append(tf.reduce_mean(valid_step(inp, tar), axis=-1))\n",
        "\n",
        "    epoch_valid_loss.append(valid_loss.result())\n",
        "    epoch_valid_accuracy.append(valid_accuracy.result())\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('valid_loss', data=tf.reduce_mean(epoch_valid_loss), step=epoch)\n",
        "    tf.summary.scalar('valid_mrr', data=tf.reduce_mean(epoch_valid_mrr), step=epoch)\n",
        "    tf.summary.scalar('valid_accuracy', data=tf.reduce_mean(epoch_valid_accuracy), step=epoch)\n",
        "\n",
        "  print('Epoch {} Train Loss {:.10f} Train Accuracy {:.10f} Valid Loss {:.10f} Valid Accuracy {:.10f} Valid MRR {:.10f}'.format(epoch + 1, \n",
        "                                                tf.reduce_mean(epoch_train_loss), \n",
        "                                                tf.reduce_mean(epoch_train_accuracy),\n",
        "                                                tf.reduce_mean(epoch_valid_loss), \n",
        "                                                tf.reduce_mean(epoch_valid_accuracy),\n",
        "                                                tf.reduce_mean(epoch_valid_mrr)))\n",
        "\n",
        "  print('Time taken for 1 epoch: {} seconds\\n'.format(time.time() - start))\n",
        "  stopwatch.append(time.time() - start)\n",
        "\n",
        "  if tf.reduce_mean(epoch_valid_mrr) > best_valid_mrr:\n",
        "    best_valid_mrr = tf.reduce_mean(epoch_valid_mrr)\n",
        "\n",
        "    ckpt_manager.save()\n",
        "    print('Model saved at epoch {}\\n'.format(epoch+1))\n",
        "\n",
        "print('Total training time: {} seconds\\n'.format(tf.reduce_sum(stopwatch)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALjDin80RJNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb sync logs/gradient_tape/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpXW8lYubuMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/gradient_tape/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmNW1kMlnNdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_special(data):\n",
        "  for index, row in enumerate(data):\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_empty(data):\n",
        "  for index, row in enumerate(data):\n",
        "    for token in row:\n",
        "      if not token:  \n",
        "        # removing empty strings from the list.\n",
        "        data[index] = list(filter(None, row))\n",
        "\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izLvz4l8nN-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/queries.txt', 'r') as f:\n",
        "    queries_file = f.readlines()\n",
        "\n",
        "queries = [[line.strip()] for line in queries_file]\n",
        "queries = [token.split() for line in queries for token in line]\n",
        "queries = remove_special(queries)\n",
        "queries = remove_empty(queries)\n",
        "\n",
        "encoded_queries, _ = encode(queries, [], input_encoder, None)\n",
        "encoded_queries, _ = to_numpy(encoded_queries, [])\n",
        "\n",
        "queries_set = tf.constant(encoded_queries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqwHMveMnPeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padding_mask_inp = 1 - tf.cast(tf.equal(query_set, 0), dtype=tf.float32)\n",
        "padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "_, query_representations, _ = matching_network(query_set, query_set, padding_mask_inp, padding_mask_inp, False, False)\n",
        "\n",
        "function_representations = []\n",
        "\n",
        "for inp, tar in train_dataset:\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  _, _, function_vectors = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, False, False)\n",
        "\n",
        "  function_representations.append(function_vectors)\n",
        "\n",
        "function_representations = tf.concat(function_representations, axis=-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wzcuTrgnRGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = AnnoyIndex(tf.shape(function_representations)[-1], 'angular')\n",
        "\n",
        "for index, vector in enumerate(function_representations):\n",
        "  indices.add_item(index, vector)\n",
        "\n",
        "indices.build(10)\n",
        "indices.save('/content/drive/My Drive/Cosine/functions.ann')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6oU_FuJnSLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(vector, indices):\n",
        "  function_index, distance = indices.get_nns_by_vector(vector, n=100, include_distances=True)\n",
        "\n",
        "  return function_index, distance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSxQAWL_nTkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for query_index, vector in enumerate(query_representations):\n",
        "  function_index, distance = get_predictions(vector, indices)\n",
        "  \n",
        "  for index in function_index:\n",
        "    predictions.append([queries_file[query_index], 'java', dataset.url[index]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1IDFE5nU0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions_dataframe = pd.DataFrame(predictions, columns=['query', 'language', 'url'])\n",
        "predictions_dataframe.to_csv('/content/drive/My Drive/COSINE/model_predictions.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJoR5ZLMI1dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing section.\n",
        "i = 0\n",
        "\n",
        "for inp, tar in valid_dataset:\n",
        "  padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "  padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "  padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  out, en1, en2 = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, False)\n",
        "\n",
        "  i += 1\n",
        "  if i == 1:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZQVPussly_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uku-igHSoi7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t1 = en1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfovqgTVHf0h",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "t2 = en2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqSmdhYmIUUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cosine = Cosine()\n",
        "result = cosine(t1, t2)\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}