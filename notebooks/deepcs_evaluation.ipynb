{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "deepcs_evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MJouRug7Adc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZQgL9jr_A8"
      },
      "source": [
        "Importing libraries and APIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSfzHqa6Sm4f"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import ast\n",
        "import csv\n",
        "import math as m\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUeVf8H0uh_4"
      },
      "source": [
        "Defining the raw input preprocessing functions.\n",
        "*   *remove_special*: replaces all special characters in the raw input with an empty string.\n",
        "*   *remove_empty*: removes all empty strings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nTJAvtPVcUj"
      },
      "source": [
        "def remove_special(data):\n",
        "  for index, row in enumerate(data):\n",
        "    for token in row:\n",
        "      token_index = row.index(token)\n",
        "      # replacing special characters with an empty string.\n",
        "      token = re.sub(r'[^A-Za-z0-9]+', '', token)\n",
        "      data[index][token_index] = token\n",
        "\n",
        "  return data\n",
        "\n",
        "def remove_empty(data):\n",
        "  for index, row in enumerate(data):\n",
        "    for token in row:\n",
        "      if not token:  \n",
        "        # removing empty strings from the list.\n",
        "        data[index] = list(filter(None, row))\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA2rb29vukHt"
      },
      "source": [
        "Reading the 50 queries from a TXT file and preprocessing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5LzWSGyTTV_"
      },
      "source": [
        "with open('/content/drive/MyDrive/queries.txt', 'r') as f:\n",
        "  queries_file = f.readlines()\n",
        "\n",
        "# tokenizing the queries.\n",
        "queries = [token.split() for line in queries_file for token in [line.strip()]]\n",
        "queries = remove_special(queries)\n",
        "queries = remove_empty(queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-fd2-E0sEF4"
      },
      "source": [
        "Loading the dataset and the vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfF4zQM_SrzP"
      },
      "source": [
        "# loading the docstring vocabulary.\n",
        "docstring_vocab =  pickle.load(open('/content/drive/MyDrive/ΔΙΠΛΩΜΑΤΙΚΗ ΕΡΓΑΣΙΑ/ΔΕΔΟΜΕΝΑ/code_vocab.pkl', 'rb'))\n",
        "# loading the code vocabulary.\n",
        "code_vocab =  pickle.load(open('/content/drive/MyDrive/ΔΙΠΛΩΜΑΤΙΚΗ ΕΡΓΑΣΙΑ/ΔΕΔΟΜΕΝΑ/code_vocab.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6kCaF4fsIWj"
      },
      "source": [
        "Defining the encoding functions.\n",
        "\n",
        "*   *encode*: encodes all words in a dataset to integers.\n",
        "*   *to_numpy*: converts the dataframes to numpy and pads them with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELRc6XXkNyQS"
      },
      "source": [
        "def encode(inp, tar, input_encoder, target_encoder):\n",
        "  # encoding input data.\n",
        "  for index, row in enumerate(inp):\n",
        "    inp[index] = [input_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  # encoding target data.\n",
        "  for index, row in enumerate(tar):\n",
        "    tar[index] = [target_encoder.encode(token)[0] for token in row]\n",
        "\n",
        "  return inp, tar\n",
        "\n",
        "def to_numpy(inp, tar):\n",
        "  # converting input data to numpy.\n",
        "  inp = pd.DataFrame(list(inp))\n",
        "  inp = inp.to_numpy()\n",
        "  inp = np.nan_to_num(inp)\n",
        "  inp = inp.astype(int)\n",
        "\n",
        "  # converting target data to numpy.\n",
        "  tar = pd.DataFrame(list(tar))\n",
        "  tar = tar.to_numpy()\n",
        "  tar = np.nan_to_num(tar)\n",
        "  tar = tar.astype(int)\n",
        "\n",
        "  return inp, tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhcfZJSsHkW"
      },
      "source": [
        "Splitting the dataset to training and validation sets and applying the encoding functions to the docstring and function tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLmqTMk4NzTQ"
      },
      "source": [
        "# building input_encoder.\n",
        "input_encoder = tfds.deprecated.text.TokenTextEncoder(docstring_vocab)\n",
        "# building target_encoder.\n",
        "target_encoder = tfds.deprecated.text.TokenTextEncoder(code_vocab)\n",
        "\n",
        "# applying encoding to the queries and converting them to numpy.\n",
        "encoded_queries, _ = encode(queries, [], input_encoder, None)\n",
        "encoded_queries, _ = to_numpy(encoded_queries, [])\n",
        "# creating a constant tensor with the encoded queries.\n",
        "encoded_queries = tf.constant(encoded_queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cu-R3Rv9iI6"
      },
      "source": [
        "input_encoder.decode(encoded_queries[14])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF4J5F2MsPYv"
      },
      "source": [
        "Creating the Positional Encoding layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5jo2SY2cbAo"
      },
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, position, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    # creating the positional encoding matrix.\n",
        "    self.pe = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    # storing word positions to a matrix.\n",
        "    position = tf.range(position, dtype=tf.float32)[:, tf.newaxis]\n",
        "    # storing embedding components to a matrix.\n",
        "    i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
        "\n",
        "    # calculating the angles.\n",
        "    angle = tf.multiply(position, 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, dtype=tf.float32)))\n",
        "\n",
        "    # applying sine to the angles of even indices.\n",
        "    sines = tf.sin(angle[:, 0::2])\n",
        "    # applying cosine to the angles of odd indices.\n",
        "    cosines = tf.cos(angle[:, 1::2])\n",
        "\n",
        "    # concatenating sines and cosines in one matrix.\n",
        "    pe = tf.concat([sines, cosines], axis=-1)[tf.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pe, dtype=tf.float32)\n",
        "\n",
        "  def call(self, x):\n",
        "    # adding positional encoding to the input embeddings on call.\n",
        "    return x + self.pe[:, :tf.shape(x)[-2], :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2O2WkqUsRw_"
      },
      "source": [
        "Creating the Multi-Head Attention layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tTskI9ccaS"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, d_model):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    self.head_size = d_model // num_heads\n",
        "\n",
        "    # creating the weight matrices for each head.\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # creating the weight matrix for the output.\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, query, key, value, mask):\n",
        "    # storing the batch size.\n",
        "    batch_size = tf.shape(query)[-3]\n",
        "\n",
        "    # passing query, key and value as input to the weight matrices.\n",
        "    query = self.wq(query)\n",
        "    key = self.wk(key)\n",
        "    value = self.wv(value)\n",
        "\n",
        "    # splitting the dense tensors for each head.\n",
        "    query = tf.reshape(query, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    key = tf.reshape(key, [batch_size, -1, self.num_heads, self.head_size])\n",
        "    value = tf.reshape(value, [batch_size, -1, self.num_heads, self.head_size])\n",
        "\n",
        "    # transposing the number of heads and sequence length columns.\n",
        "    query = tf.transpose(query, perm=[0, 2, 1, 3])\n",
        "    key = tf.transpose(key, perm=[0, 2, 1, 3])\n",
        "    value = tf.transpose(value, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # calculating the similarity score.\n",
        "    query_keyT = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # calculating the depth.\n",
        "    depth = tf.cast(tf.shape(key)[-1], dtype=tf.float32)\n",
        "    # calculating the scale factor.\n",
        "    scale = 1 / tf.sqrt(depth)\n",
        "\n",
        "    # calculating the scaled similarity scores.\n",
        "    scores = query_keyT * scale\n",
        "\n",
        "    # masking out key/value pairs.\n",
        "    if mask is not None:\n",
        "      scores *= mask\n",
        "      scores = tf.where(tf.equal(scores, 0), tf.ones_like(scores) * -1e9, scores)\n",
        "\n",
        "    # calculating the scaled similarity scores' softmax matrix.\n",
        "    softmax = tf.nn.softmax(scores)\n",
        "\n",
        "    # calculating the scaled dot-product attention for each head.\n",
        "    attention = tf.matmul(softmax, value)\n",
        "    attention = tf.transpose(attention, [0, 2, 1, 3])\n",
        "\n",
        "    # concatenating the attention heads.\n",
        "    output = tf.reshape(attention, [batch_size, -1, self.d_model])\n",
        "    # passing the concatenation as input to a dense layer.\n",
        "    output = self.dense(output)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCUbMKGIsUS0"
      },
      "source": [
        "Creating the Feed Forward Network layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyoVUxxrcd41"
      },
      "source": [
        "class FeedForwardNetwork(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, dff, d_model):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "\n",
        "    # creating the dense layers of the feed forward network.\n",
        "    self.fc1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.fc2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, x):\n",
        "    # applying the layer with dff units and relu activation.\n",
        "    fc1 = self.fc1(x)\n",
        "    # applying the layer with d_model units and no activation.\n",
        "    output = self.fc2(fc1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyG-OV7XsVwP"
      },
      "source": [
        "Creating the Encoder layer of the Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIACE_slcfAA"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_heads, dff, d_model, rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    # creating the MHA and FFN layers.\n",
        "    self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "    self.ffn = FeedForwardNetwork(dff, d_model)\n",
        "\n",
        "    # creating the dropout layers.\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the normalization layers.\n",
        "    self.normalization1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normalization2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # applying multi-head attention.\n",
        "    attention = self.mha(x, x, x, padding_mask)\n",
        "    dropout1 = self.dropout1(attention, training=training)\n",
        "    normalization1 = self.normalization1(x + dropout1)\n",
        "\n",
        "    # applying the feed forward network.\n",
        "    ffn = self.ffn(normalization1)\n",
        "    dropout2 = self.dropout2(ffn, training=training)\n",
        "    output = self.normalization2(normalization1 + dropout2)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQwXR1u8sX3i"
      },
      "source": [
        "Creating the Encoder layer of the Matching Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5rvsH0JcgiC"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, num_layers, vocab_size, position, num_heads, dff, d_model, rate):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # creating the embedding and positional encoding layers.\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "    self.pe = PositionalEncoding(position, d_model)\n",
        "\n",
        "    # creating the dropout layer.    \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    # creating the encoder layers.    \n",
        "    self.encoder_layers = [EncoderLayer(num_heads, dff, d_model, rate) for index in range(num_layers)]\n",
        "\n",
        "  def call(self, x, padding_mask, training):\n",
        "    # calculating the embeddings and applying the positional encoding.\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
        "    x = self.pe(x)\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for index in range(self.num_layers):\n",
        "      # stacking the encoder layers.\n",
        "      x = self.encoder_layers[index](x, padding_mask, training)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNs400xzs0Md"
      },
      "source": [
        "Creating the Triangle's Area - Sector's Area Similarity layer of the Matching Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AbXDVps3XR0"
      },
      "source": [
        "class TSSS(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super(TSSS, self).__init__()\n",
        "\n",
        "  def call(self, x, y):\n",
        "    # calculating the theta angle.\n",
        "    x_normalized = tf.math.l2_normalize(x, axis=-1)\n",
        "    y_normalized = tf.math.l2_normalize(y, axis=-1)\n",
        "    theta = tf.acos(tf.matmul(x_normalized, y_normalized, transpose_b=True)) + tf.constant(m.radians(10))\n",
        "\n",
        "    # calculating the euclidean distance.\n",
        "    x_matrix = tf.reshape(tf.repeat(x, repeats=tf.shape(x)[-2], axis=-2), shape=[-1, tf.shape(x)[-2], tf.shape(x)[-1]])\n",
        "    y_matrix = tf.reshape(tf.tile(y, multiples=[tf.shape(y)[-2], 1]), shape=[-1, tf.shape(y)[-2], tf.shape(y)[-1]])\n",
        "    ed = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(x_matrix, y_matrix)), axis=-1))\n",
        "\n",
        "    # calculating the magnitude difference.\n",
        "    x_sqrt = tf.sqrt(tf.reduce_sum(tf.square(x_matrix), axis=-1))\n",
        "    y_sqrt = tf.sqrt(tf.reduce_sum(tf.square(y_matrix), axis=-1))\n",
        "    md = tf.abs(x_sqrt - y_sqrt)\n",
        "\n",
        "    # calculating the Triangle's Area Similarity.\n",
        "    x_norm = tf.norm(x, ord='euclidean', axis=-1)[:, tf.newaxis]\n",
        "    y_norm = tf.norm(y, ord='euclidean', axis=-1)[:, tf.newaxis]\n",
        "    ts = (tf.matmul(x_norm, y_norm, transpose_b=True) * tf.sin(theta * (tf.constant(m.pi) / 180))) / 2\n",
        "\n",
        "    # calculating the Sector's Area Similarity.\n",
        "    ss = (tf.constant(m.pi) * tf.pow((ed + md), 2) * theta) / 360\n",
        "\n",
        "    # calculating the TS-SS.\n",
        "    output = ts * ss\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQq0sgHVs5k8"
      },
      "source": [
        "Creating the model of the Matching Network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3AZ1H7I_SA4"
      },
      "source": [
        "class MatchingNetwork(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, num_layers, input_vocab_size, target_vocab_size, input_position, target_position, num_heads, dff, d_model, rate):\n",
        "    super(MatchingNetwork, self).__init__()\n",
        "\n",
        "    # creating the Encoders.\n",
        "    self.encoder1 = Encoder(num_layers, input_vocab_size, input_position, num_heads, dff, d_model, rate)\n",
        "    self.encoder2 = Encoder(num_layers, target_vocab_size, target_position, num_heads, dff, d_model, rate)\n",
        "\n",
        "    # creating the Triangle Sector Similarity layer.\n",
        "    self.similarity = TSSS()\n",
        "\n",
        "  def call(self, x, y, padding_mask_x, padding_mask_y, training):\n",
        "    # creating the encoded input padding mask.\n",
        "    mask1 = tf.squeeze(padding_mask_x)[:, :, tf.newaxis]\n",
        "    mask1 = -1e9 * (1 - mask1)\n",
        "\n",
        "    # passing the input data to its corresponding encoder.\n",
        "    encoded1 = self.encoder1(x, padding_mask_x, training)\n",
        "    encoded1 = encoded1 + mask1\n",
        "    encoded1 = tf.reduce_max(encoded1, axis=-2)\n",
        "\n",
        "    # creating the encoded target padding mask.\n",
        "    mask2 = tf.squeeze(padding_mask_y)[:, :, tf.newaxis]\n",
        "    mask2 = -1e9 * (1 - mask2)\n",
        "\n",
        "    # passing the target data to its corresponding encoder.\n",
        "    encoded2 = self.encoder2(y, padding_mask_y, training)\n",
        "    encoded2 = encoded2 + mask2\n",
        "    encoded2 = tf.reduce_max(encoded2, axis=-2)\n",
        "\n",
        "    # calculating the similarity and its invert.\n",
        "    similarity = self.similarity(encoded1, encoded2)\n",
        "    similarity = 1 / similarity\n",
        "\n",
        "    return similarity, encoded1, encoded2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlZUupx8tMVf"
      },
      "source": [
        "Creating the Matching Network with specific hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGb49npMa_Qd"
      },
      "source": [
        "NUM_LAYERS = 3\n",
        "INPUT_VOCAB_SIZE = input_encoder.vocab_size\n",
        "TARGET_VOCAB_SIZE = target_encoder.vocab_size\n",
        "INPUT_POSITION = input_encoder.vocab_size\n",
        "TARGET_POSITION = target_encoder.vocab_size\n",
        "NUM_HEADS = 8\n",
        "DFF = 512\n",
        "D_MODEL = 128\n",
        "RATE = 0.1\n",
        "\n",
        "matching_network = MatchingNetwork(NUM_LAYERS, INPUT_VOCAB_SIZE, TARGET_VOCAB_SIZE,\n",
        "                                   INPUT_POSITION, TARGET_POSITION, NUM_HEADS,\n",
        "                                   DFF, D_MODEL, RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM-tXmAPItkd"
      },
      "source": [
        "Loading model's weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUZPyb9HIt84"
      },
      "source": [
        "if os.path.isfile('/content/drive/MyDrive/ΔΙΠΛΩΜΑΤΙΚΗ ΕΡΓΑΣΙΑ/ΥΛΟΠΟΙΗΣΗ/ΑΠΟΤΕΛΕΣΜΑΤΑ/weights.index'):\n",
        "  matching_network.load_weights('/content/drive/MyDrive/ΔΙΠΛΩΜΑΤΙΚΗ ΕΡΓΑΣΙΑ/ΥΛΟΠΟΙΗΣΗ/ΑΠΟΤΕΛΕΣΜΑΤΑ/weights')\n",
        "  print('Model restored.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCqwRZNImA-W"
      },
      "source": [
        "Creating a faster implementatio of TS-SS to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3MDFT_PNZU3"
      },
      "source": [
        "def fast_tsss(x, y):\n",
        "  ed = np.linalg.norm(x - y)\n",
        "  md = np.abs(np.linalg.norm(x) - np.linalg.norm(y))\n",
        "\n",
        "  cosine = np.dot(x, y.T) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "  theta = np.arccos(cosine) + np.radians(10)\n",
        "\n",
        "  ts = (np.linalg.norm(x) * np.linalg.norm(y) * np.sin(np.radians(theta))) / 2\n",
        "\n",
        "  ss = (m.pi * (ed + md)**2 * theta) / 360\n",
        "\n",
        "  tsss = ts * ss\n",
        "\n",
        "  return tsss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtGlTlNFlrz_"
      },
      "source": [
        "Creating and storing the vector representations of the 50 queries and the whole corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFgLGTLGULiD"
      },
      "source": [
        "# creating the input padding mask.\n",
        "padding_mask_inp = 1 - tf.cast(tf.equal(encoded_queries, 0), dtype=tf.float32)\n",
        "padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "_, query_representations, _ = matching_network(encoded_queries, encoded_queries, padding_mask_inp, padding_mask_inp, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmnW8-dVmGhL"
      },
      "source": [
        "Evaluating CodeTransformer with DeepCS in RAM and storage friendly manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdVuOBONSg2i"
      },
      "source": [
        "for deepcs_tokens in pd.read_csv('/content/drive/MyDrive/deepcs_tokens.csv', header=None, squeeze=True, chunksize=1000000):\n",
        "  deepcs_tokens = deepcs_tokens.apply(ast.literal_eval)\n",
        "  deepcs_tokens = deepcs_tokens.tolist()\n",
        "  \n",
        "  # applying encoding to the DeepCS corpus and converting it to numpy.\n",
        "  _, encoded_deepcs = encode([], deepcs_tokens, None, target_encoder)\n",
        "  _, encoded_deepcs = to_numpy([], encoded_deepcs)\n",
        "\n",
        "  # creating a tensor dataset with the whole corpus.\n",
        "  encoded_deepcs = tf.data.Dataset.from_tensor_slices(encoded_deepcs)\n",
        "  # caching the dataset for performance optimizations.\n",
        "  encoded_deepcs = encoded_deepcs.cache()\n",
        "  encoded_deepcs = encoded_deepcs.batch(1000)\n",
        "  encoded_deepcs = encoded_deepcs.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  function_representations = []\n",
        "\n",
        "  for tar in encoded_deepcs:\n",
        "    inp = tf.ones_like(tar)\n",
        "\n",
        "    # creating the input padding mask.\n",
        "    padding_mask_inp = 1 - tf.cast(tf.equal(inp, 0), dtype=tf.float32)\n",
        "    padding_mask_inp = padding_mask_inp[:, tf.newaxis, tf.newaxis, :]\n",
        "    # creating the target padding mask.\n",
        "    padding_mask_tar = 1 - tf.cast(tf.equal(tar, 0), dtype=tf.float32)\n",
        "    padding_mask_tar = padding_mask_tar[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    _, _, function_vectors = matching_network(inp, tar, padding_mask_inp, padding_mask_tar, False)\n",
        "\n",
        "    # storing the function vectors in a list.\n",
        "    function_representations.append(function_vectors)\n",
        "\n",
        "  # concatenating all vectors on the first axis.\n",
        "  function_representations = tf.concat(function_representations, axis=-2)\n",
        "\n",
        "  for index, query in enumerate(query_representations):\n",
        "    filepath = '/content/drive/MyDrive/Results/' + str(index) + '.csv'\n",
        "\n",
        "    with open(filepath, 'a') as f:\n",
        "      writer = csv.writer(f)\n",
        "\n",
        "      for function in function_representations:\n",
        "        writer.writerow([fast_tsss(query.numpy(), function.numpy())])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}